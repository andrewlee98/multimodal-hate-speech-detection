{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "# from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ckpt_dir = os.path.join(\"../bert_models/pretrained_bert_model\")\n",
    "bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\") #.data-00000-of-00001\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1]\n",
    "def create_model(max_seq_len, bert_ckpt_file):\n",
    "\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = None\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n",
    "  bert_output = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", bert_output.shape)\n",
    "\n",
    "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "#   cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "#   logits = keras.layers.Dropout(0.5)(logits)\n",
    "#   logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n",
    "  logits = keras.layers.Dense(units=1, activation=\"sigmoid\")(logits)\n",
    "\n",
    "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 60, 768)\n",
      "Done loading 196 BERT weights from: ../bert_models/pretrained_bert_model/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f20bf864c50> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    }
   ],
   "source": [
    "# model = create_model(data.max_seq_len, bert_ckpt_file)\n",
    "model = create_model(60, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 60)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 60, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 109,481,473\n",
      "Trainable params: 109,481,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "data_dir = '../facebook_challenge_data/'\n",
    "model_dir = 'models/'\n",
    "\n",
    "# load data and print sizes\n",
    "# load data and print sizes\n",
    "def get_dict(path):\n",
    "    jsonl_content = open(path, 'r').read()\n",
    "    data = [json.loads(jline) for jline in jsonl_content.split('\\n')]\n",
    "    return {datum['id'] : datum for datum in data}\n",
    "\n",
    "\n",
    "train_dict = get_dict(data_dir + 'train.jsonl')\n",
    "val_dict = get_dict(data_dir + 'dev.jsonl')\n",
    "test_dict = get_dict(data_dir + 'test.jsonl')\n",
    "\n",
    "print(len(train_dict))\n",
    "print(len(val_dict))\n",
    "print(len(test_dict))\n",
    "\n",
    "def get_text_data(dictionary):\n",
    "    return [(datum['text'], datum['label']) for datum in dictionary.values()]\n",
    "\n",
    "train_data = get_text_data(train_dict)\n",
    "val_data = get_text_data(val_dict)\n",
    "# test_data = get_text_data(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its their character not their color that matters ['its', 'their', 'character', 'not', 'their', 'color', 'that', 'matters']\n",
      "[2049, 2037, 2839, 2025, 2037, 3609, 2008, 5609]\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer from bert library\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "print(train_data[0][0], tokenizer.tokenize(train_data[0][0]))\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8500/8500 [00:01<00:00, 5457.30it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5958.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (8500, 60)\n",
      "Shape of label tensor: (8500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sequences\n",
    "# https://colab.research.google.com/drive/1WQY_XxdiCVFzjMXnDdNfUjDFi0CN5hkT#scrollTo=TApTW_wLxoA9\n",
    "\n",
    "MAX_SEQ_LEN = 50\n",
    "\n",
    "def list_to_mats(l, tokenizer, pad_len):\n",
    "    texts, labels = zip(*l)\n",
    "    \n",
    "    sequences = []\n",
    "    for text in tqdm(texts):\n",
    "        tokens = [\"[CLS]\"] + tokenizer.tokenize(text) + [\"[SEP]\"]\n",
    "        tokens = [t for t in tokens if t[0]!='<'] # remove twitter specific text\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        sequences.append(token_ids)\n",
    "    \n",
    "    x = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=60, padding='post')\n",
    "    y = np.asarray(labels)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = list_to_mats(train_data, tokenizer, MAX_SEQ_LEN)\n",
    "x_val, y_val = list_to_mats(val_data, tokenizer, MAX_SEQ_LEN)\n",
    "# x_test, y_test = list_to_mats(test_data, tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "532/532 [==============================] - 631s 1s/step - loss: 0.7010 - accuracy: 0.6174 - val_loss: 0.8762 - val_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "532/532 [==============================] - 622s 1s/step - loss: 0.6725 - accuracy: 0.6222 - val_loss: 0.7904 - val_accuracy: 0.5000\n",
      "Epoch 3/15\n",
      "532/532 [==============================] - 626s 1s/step - loss: 0.6717 - accuracy: 0.6180 - val_loss: 0.9277 - val_accuracy: 0.5000\n",
      "Epoch 4/15\n",
      "532/532 [==============================] - 624s 1s/step - loss: 0.6680 - accuracy: 0.6262 - val_loss: 0.7055 - val_accuracy: 0.5000\n",
      "Epoch 5/15\n",
      "532/532 [==============================] - 622s 1s/step - loss: 0.6779 - accuracy: 0.6153 - val_loss: 0.7480 - val_accuracy: 0.5000\n",
      "Epoch 6/15\n",
      "532/532 [==============================] - 619s 1s/step - loss: 0.6706 - accuracy: 0.6260 - val_loss: 0.7008 - val_accuracy: 0.5000\n",
      "Epoch 7/15\n",
      "532/532 [==============================] - 621s 1s/step - loss: 0.6691 - accuracy: 0.6269 - val_loss: 0.7129 - val_accuracy: 0.5000\n",
      "Epoch 8/15\n",
      "532/532 [==============================] - 627s 1s/step - loss: 0.6695 - accuracy: 0.6268 - val_loss: 0.7522 - val_accuracy: 0.5000\n",
      "Epoch 9/15\n",
      "532/532 [==============================] - 622s 1s/step - loss: 0.6766 - accuracy: 0.6146 - val_loss: 0.8120 - val_accuracy: 0.5000\n",
      "Epoch 10/15\n",
      "532/532 [==============================] - 622s 1s/step - loss: 0.6719 - accuracy: 0.6249 - val_loss: 0.7349 - val_accuracy: 0.5000\n",
      "Epoch 11/15\n",
      "532/532 [==============================] - 627s 1s/step - loss: 0.6703 - accuracy: 0.6247 - val_loss: 0.7548 - val_accuracy: 0.5000\n",
      "Epoch 12/15\n",
      "532/532 [==============================] - 629s 1s/step - loss: 0.6769 - accuracy: 0.6179 - val_loss: 0.7854 - val_accuracy: 0.5000\n",
      "Epoch 13/15\n",
      "532/532 [==============================] - 630s 1s/step - loss: 0.6707 - accuracy: 0.6271 - val_loss: 0.7554 - val_accuracy: 0.5000\n",
      "Epoch 14/15\n",
      "532/532 [==============================] - 629s 1s/step - loss: 0.6715 - accuracy: 0.6231 - val_loss: 0.6970 - val_accuracy: 0.5000\n",
      "Epoch 15/15\n",
      "532/532 [==============================] - 630s 1s/step - loss: 0.6702 - accuracy: 0.6282 - val_loss: 0.7334 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# shuffle data\n",
    "indices = (np.arange(x_train.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "x_train, y_train = x_train[indices], y_train[indices]\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=15, batch_size=16, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.496568\n",
      "Val acc: 0.5\n",
      "Val F1: 0.0\n",
      "Val Precision: 1.0\n",
      "Val Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# get AUROC\n",
    "preds = model.predict(x_val)\n",
    "print('Test AUROC:', roc_auc_score(y_val, preds))\n",
    "\n",
    "# get loss and acc\n",
    "preds_bin = np.array(preds)\n",
    "preds_bin[preds >= 0.5] = 1\n",
    "preds_bin[preds < 0.5] = 0\n",
    "print('Val acc:', accuracy_score(y_val, preds_bin))\n",
    "\n",
    "# get F1\n",
    "print('Val F1:', f1_score(y_val, preds_bin, zero_division=1))\n",
    "print('Val Precision:', precision_score(y_val, preds_bin, zero_division=1))\n",
    "print('Val Recall:', recall_score(y_val, preds_bin, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.5\n",
      "Test F1: 0.6666666666666666\n",
      "Test Precision: 0.5\n",
      "Test Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "# accuracy scores with different threshold\n",
    "\n",
    "preds_bin[preds>0.2] = 1\n",
    "preds_bin[preds<=0.2] = 0\n",
    "print('Test acc:', accuracy_score(y_val, preds_bin))\n",
    "print('Test F1:', f1_score(y_val, preds_bin, zero_division=1))\n",
    "print('Test Precision:', precision_score(y_val, preds_bin, zero_division=1))\n",
    "print('Test Recall:', recall_score(y_val, preds_bin, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(preds)\n",
    "plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011173248291, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609012961387634, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011173248291, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.36090123653411865, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011471271515, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090123653411865, 0.36090123653411865, 0.36090120673179626, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090126633644104, 0.3609011769294739, 0.3609011471271515, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609011471271515, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011173248291, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.3609011173248291, 0.3609011471271515, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.36090126633644104, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090126633644104, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011173248291, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011173248291, 0.3609011769294739, 0.36090126633644104, 0.36090120673179626, 0.36090123653411865, 0.36090123653411865, 0.3609011769294739, 0.3609011471271515, 0.36090120673179626, 0.36090123653411865, 0.36090120673179626, 0.36090120673179626, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011173248291, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.36090123653411865, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090123653411865, 0.36090123653411865, 0.36090120673179626, 0.3609011173248291, 0.36090123653411865, 0.36090120673179626, 0.36090126633644104, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090126633644104, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011173248291, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090120673179626, 0.3609011471271515, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090123653411865, 0.3609011471271515, 0.3609011769294739, 0.36090126633644104, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609012961387634, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.3609011769294739, 0.3609011471271515, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.3609011769294739, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.3609011471271515, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.36090123653411865, 0.36090120673179626, 0.3609011769294739, 0.36090120673179626, 0.36090120673179626, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.36090123653411865, 0.3609011769294739, 0.3609011769294739]\n"
     ]
    }
   ],
   "source": [
    "print([n.item() for n in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37env",
   "language": "python",
   "name": "python37env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# create giant dictionary for all data\n",
    "data_dir = '../facebook_challenge_data/'\n",
    "model_dir = 'models/'\n",
    "\n",
    "# load data and print sizes\n",
    "def get_dict(path):\n",
    "    jsonl_content = open(path, 'r').read()\n",
    "    data = [json.loads(jline) for jline in jsonl_content.split('\\n')]\n",
    "    return {datum['id'] : datum for datum in data}\n",
    "\n",
    "\n",
    "train_dict = get_dict(data_dir + 'train.jsonl')\n",
    "val_dict = get_dict(data_dir + 'dev.jsonl')\n",
    "test_dict = get_dict(data_dir + 'test.jsonl')\n",
    "\n",
    "print(len(train_dict))\n",
    "print(len(val_dict))\n",
    "print(len(test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint # for random cropping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class FBMMDataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_dict, tokenizer, pad_len, batch_size=32, dim=(299, 299), n_channels=3, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.data_dict = data_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.pad_len = pad_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # build labels list and id list\n",
    "        self.id_list = list(self.data_dict.keys())\n",
    "        self.labels = {ID: self.data_dict[ID]['label'] for ID in self.id_list}\n",
    "        self.img_list = {ID: self.data_dict[ID]['img'] for ID in self.id_list}\n",
    "            \n",
    "        # get text dictionary\n",
    "        self.text_dict = self.process_text(self.id_list)\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        self.classes = [self.labels[self.id_list[i]] for i in self.indexes]\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.id_list) / self.batch_size)) + 1 # last batch is partial\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:index*self.batch_size + self.batch_size]\n",
    "        \n",
    "        \n",
    "        # Find list of IDs\n",
    "        id_list_temp = [self.id_list[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X_txt, X_img, y = self.__data_generation(id_list_temp)\n",
    "        \n",
    "        return (X_txt, X_img), y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.id_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, id_list_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X_img = np.empty((len(id_list_temp), *self.dim, self.n_channels))\n",
    "        X_txt = np.empty((len(id_list_temp), self.pad_len))\n",
    "        y = np.empty(len(id_list_temp), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(id_list_temp):\n",
    "            # Store sample\n",
    "            X_img[i,] = self.process_img(data_dir + self.img_list[ID])\n",
    "            X_txt[i,] = self.text_dict[ID]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X_txt.astype(int), X_img, y\n",
    "    \n",
    "    def process_img(self, path): # method for getting image\n",
    "        img = Image.open(path)\n",
    "        img.load()\n",
    "        img = img.resize(self.dim, Image.ANTIALIAS)\n",
    "        data = np.asarray(img, dtype='uint8')\n",
    "        im = self.augment(data)\n",
    "        \n",
    "        \n",
    "        if im.shape==(self.dim[0], self.dim[1]): im = np.stack((im,)*3, axis=-1) # handle grayscale\n",
    "        if im.shape == (*self.dim, 4): im = im[:,:,:3] # handle weird case\n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def augment(self, im): # random crop and random mirror\n",
    "        \n",
    "        # random crop\n",
    "        x_max, y_max = im.shape[0], im.shape[1]\n",
    "        x_start, y_start = randint(0, x_max - self.dim[0]), randint(0, y_max - self.dim[1])\n",
    "        im = im[x_start:x_start + self.dim[0], y_start:y_start + self.dim[1]]\n",
    "        \n",
    "        # random mirror\n",
    "        if randint(0,1): im = np.flip(im, axis=1)\n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def process_text(self, id_list):\n",
    "        \n",
    "        # matrix for texts\n",
    "        texts = [self.data_dict[ID]['text'] for ID in id_list]\n",
    "        sequences = [self.tokenizer.encode(text) for text in texts] # make this more efficient...\n",
    "        text_seqs = pad_sequences(sequences, maxlen=self.pad_len)\n",
    "        \n",
    "        id_to_seq = {ID: txt for (ID, txt) in zip(id_list, text_seqs)} # map id to text seq\n",
    "        \n",
    "        return id_to_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, TFRobertaModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "max_len = 50\n",
    "\n",
    "# create data generators\n",
    "train_gen = FBMMDataGenerator(data_dict=train_dict,\n",
    "                          tokenizer=tokenizer,\n",
    "                          pad_len=max_len,\n",
    "                          batch_size=16,\n",
    "                          dim=(224, 224),\n",
    "                          n_channels=3,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_gen = FBMMDataGenerator(data_dict=val_dict,\n",
    "                          tokenizer=tokenizer,\n",
    "                          pad_len=max_len,\n",
    "                          batch_size=16,\n",
    "                          dim=(224, 224),\n",
    "                          n_channels=3,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load resnet\n",
    "# resnet = tf.keras.applications.resnet.ResNet152(include_top=False, \n",
    "#                                                 weights='imagenet', \n",
    "#                                                 input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tf.config.experimental_run_functions_eagerly(True) # https://github.com/tensorflow/tensorflow/issues/34944MAX_SEQ_LEN = 50 # 50 words\n",
    "# from tensorflow.keras import backend as K\n",
    "\n",
    "# MAX_SEQ_LEN=50\n",
    "\n",
    "# # load pretrained roberta\n",
    "# import transformers\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# # load roberta\n",
    "# class ROBERTA(transformers.TFRobertaModel):\n",
    "#     def __init__(self, config, *inputs, **kwargs):\n",
    "#         super(ROBERTA, self).__init__(config, *inputs, **kwargs)\n",
    "#         self.roberta.call = tf.function(self.roberta.call)\n",
    "# roberta = ROBERTA.from_pretrained('roberta-base')\n",
    "\n",
    "# # get embedding matrix\n",
    "# text_embedding = roberta.get_input_embeddings().word_embeddings\n",
    "# print(type(text_embedding))\n",
    "\n",
    "# # handle text\n",
    "# input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32)\n",
    "# text_embeds = tf.gather(text_embedding, input_ids)\n",
    "# print(type(text_embeds))\n",
    "\n",
    "# # handle images\n",
    "# input_imgs = layers.Input(shape=(224,224,3))\n",
    "# img_embeds = resnet(input_imgs)\n",
    "# img_embeds = layers.AveragePooling2D(pool_size=(2, 2))(img_embeds)\n",
    "# print('Image embedding shape after pooling (K, M):', img_embeds.shape)\n",
    "# img_embeds = layers.Reshape((img_embeds.shape[1] * img_embeds.shape[2], img_embeds.shape[3]))(img_embeds) # KM*2048\n",
    "# print('Image embedding shape after reshape:', img_embeds.shape)\n",
    "# img_embeds = layers.Dense(768)(img_embeds)\n",
    "\n",
    "# # combine and get token type ids\n",
    "# print('Text and image embedding shapes:', text_embeds.shape, img_embeds.shape)\n",
    "# combined_embed = layers.Concatenate(axis=1)([text_embeds, img_embeds])\n",
    "# seg_ids = np.concatenate((np.zeros(MAX_SEQ_LEN, dtype=np.int8), np.ones(img_embeds.shape[1], dtype=np.int8)))\n",
    "\n",
    "# # pass through roberta\n",
    "# roberta_encodings = roberta(inputs={'inputs_embeds': combined_embed, \n",
    "#                             'attention_mask': None,\n",
    "#                              'token_type_ids': None})[0]#, token_type_ids=seg_ids)[0]\n",
    "# doc_encoding = tf.squeeze(roberta_encodings[:, 0:1, :], axis=1) # Keep [CLS] token encoding\n",
    "# doc_encoding = layers.Dropout(0.1)(doc_encoding) # Apply dropout\n",
    "# outputs = layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n",
    "# model = tf.keras.models.Model(inputs=[input_ids, input_imgs], outputs=[outputs])\n",
    "\n",
    "# # compile\n",
    "# optimizer = tf.keras.optimizers.Adam(lr=5e-6)\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ROBERTA(transformers.TFRobertaModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(ROBERTA, self).__init__(config, *inputs, **kwargs)\n",
    "        self.roberta.call = tf.function(self.roberta.call)\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.resnet = tf.keras.applications.resnet.ResNet152(include_top=False, \n",
    "                                                             weights='imagenet', \n",
    "                                                             input_shape=(224, 224, 3))\n",
    "        self.roberta = ROBERTA.from_pretrained('roberta-base')\n",
    "        \n",
    "        self.text_embedding = self.roberta.get_input_embeddings().word_embeddings\n",
    "         \n",
    "        self.pooling = layers.AveragePooling2D(pool_size=(2, 2))\n",
    "        self.reshape = layers.Reshape((3 * 3, 2048))\n",
    "        self.dense1 = layers.Dense(768)\n",
    "        \n",
    "        self.concat = layers.Concatenate(axis=1)\n",
    "        \n",
    "        self.denseout = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        text, image = inputs\n",
    "        \n",
    "#         print(text, image)\n",
    "        \n",
    "        image_emb = self.resnet(image)\n",
    "        image_emb = self.pooling(image_emb)\n",
    "        image_emb = self.reshape(image_emb)\n",
    "        image_emb = self.dense1(image_emb)\n",
    "        \n",
    "        text_emb = tf.gather(self.text_embedding, text)\n",
    "        \n",
    "        concat_emb = self.concat([text_emb, image_emb])\n",
    "        roberta_encodings = self.roberta(inputs={'inputs_embeds': concat_emb})[0]\n",
    "#         print(type(roberta_encodings))\n",
    "        doc_encoding = tf.squeeze(roberta_encodings[:, 0:1, :], axis=1)\n",
    "        \n",
    "        output = self.denseout(doc_encoding)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=5e-6)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f784638c160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MyModel.call of <__main__.MyModel object at 0x7f784638c160>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        text, image = inputs\n",
      "        \n",
      "#         print(text, image)\n",
      "        \n",
      "        image_emb = self.resnet(image)\n",
      "        image_emb = self.pooling(image_emb)\n",
      "        image_emb = self.reshape(image_emb)\n",
      "        image_emb = self.dense1(image_emb)\n",
      "        \n",
      "        text_emb = tf.gather(self.text_embedding, text)\n",
      "        \n",
      "        concat_emb = self.concat([text_emb, image_emb])\n",
      "        roberta_encodings = self.roberta(inputs={'inputs_embeds': concat_emb})[0]\n",
      "#         print(type(roberta_encodings))\n",
      "        doc_encoding = tf.squeeze(roberta_encodings[:, 0:1, :], axis=1)\n",
      "        \n",
      "        output = self.denseout(doc_encoding)\n",
      "        \n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method MyModel.call of <__main__.MyModel object at 0x7f784638c160>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method MyModel.call of <__main__.MyModel object at 0x7f784638c160>>, which Python reported as:\n",
      "    def call(self, inputs):\n",
      "        text, image = inputs\n",
      "        \n",
      "#         print(text, image)\n",
      "        \n",
      "        image_emb = self.resnet(image)\n",
      "        image_emb = self.pooling(image_emb)\n",
      "        image_emb = self.reshape(image_emb)\n",
      "        image_emb = self.dense1(image_emb)\n",
      "        \n",
      "        text_emb = tf.gather(self.text_embedding, text)\n",
      "        \n",
      "        concat_emb = self.concat([text_emb, image_emb])\n",
      "        roberta_encodings = self.roberta(inputs={'inputs_embeds': concat_emb})[0]\n",
      "#         print(type(roberta_encodings))\n",
      "        doc_encoding = tf.squeeze(roberta_encodings[:, 0:1, :], axis=1)\n",
      "        \n",
      "        output = self.denseout(doc_encoding)\n",
      "        \n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "471/532 [=========================>....] - ETA: 2:02 - loss: 0.6602 - accuracy: 0.5833"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mcp_save = ModelCheckpoint(model_dir + 'best_roberta_bt_weights.h5', \n",
    "                           save_weights_only=True, \n",
    "                           save_best_only=True, \n",
    "                           verbose=1,\n",
    "                           monitor='val_loss', \n",
    "                           mode='min')\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    shuffle=True,\n",
    "                    epochs=1,\n",
    "                    callbacks=[mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import math\n",
    "\n",
    "y_val = val_gen.classes\n",
    "\n",
    "# get AUROC\n",
    "preds = fcm_model.predict_generator(val_gen)\n",
    "print('Val AUROC:', roc_auc_score(y_val, preds))\n",
    "\n",
    "# get loss and acc\n",
    "preds_bin = np.array(preds)\n",
    "preds_bin[preds>0.5] = 1\n",
    "preds_bin[preds<=0.5] = 0\n",
    "print('Val Accuracy:', accuracy_score(y_val, preds_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36env",
   "language": "python",
   "name": "python36env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "# from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ckpt_dir = os.path.join(\"pretrained_bert_model/\")\n",
    "bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\") #.data-00000-of-00001\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1]\n",
    "def create_model(max_seq_len, bert_ckpt_file):\n",
    "\n",
    "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "      bc = StockBertConfig.from_json_string(reader.read())\n",
    "      bert_params = map_stock_config_to_params(bc)\n",
    "      bert_params.adapter_size = None\n",
    "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "  input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n",
    "  bert_output = bert(input_ids)\n",
    "\n",
    "  print(\"bert shape\", bert_output.shape)\n",
    "\n",
    "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "  logits = keras.layers.Dense(units=768, activation='relu')(cls_out)\n",
    "  logits = keras.layers.Dropout(0.5)(logits)\n",
    "#   logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n",
    "  logits = keras.layers.Dense(units=1, activation=\"sigmoid\")(logits)\n",
    "\n",
    "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "  model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "  load_stock_weights(bert, bert_ckpt_file)\n",
    "        \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 50, 768)\n",
      "Done loading 196 BERT weights from: pretrained_bert_model/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f9a9bff2390> (prefix:bert_1). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    }
   ],
   "source": [
    "# model = create_model(data.max_seq_len, bert_ckpt_file)\n",
    "model = create_model(50, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 50, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 109,481,473\n",
      "Trainable params: 109,481,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Tweet Dictionary: 149823\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "data_dir = '../mmhs150k/'\n",
    "model_dir = 'models/'\n",
    "\n",
    "# load data and print sizes\n",
    "tweet_dict = json.load(open(data_dir + 'MMHS150K_GT.json', 'r'))\n",
    "print('Length of Tweet Dictionary:', len(tweet_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for cleaning text like in https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "def hashtag(text):\n",
    "    hashtag_body = text.group()[1:]\n",
    "    if hashtag_body.isupper(): return \"<hashtag> {} \".format(hashtag_body.lower())\n",
    "    else: return ' '.join([\"<hashtag>\"] + [re.sub(r\"([A-Z])\",r\" \\1\", hashtag_body, flags=re.MULTILINE | re.DOTALL)])\n",
    "\n",
    "def allcaps(text): return text.group().lower() + ' <allcaps> '    \n",
    "\n",
    "def clean_tweet_text(t):\n",
    "    eyes = r'[8:=;]'\n",
    "    nose = r\"['`\\-]?\"\n",
    "    \n",
    "    t = re.sub(r'https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*', '<url>', t)\n",
    "    t = re.sub(r'@\\w+', '<user>', t)\n",
    "    t = re.sub(r'{}{}[)dD]+|[)dD]+{}{}'.format(eyes, nose, nose, eyes), '<smile>', t)\n",
    "    t = re.sub(r'{}{}p+\".format(eyes, nose)', '<lolface>', t)\n",
    "    t = re.sub(r'{}{}\\(+|\\)+{}{}'.format(eyes, nose, nose, eyes), '<sadface>', t)\n",
    "    t = re.sub(r'{}{}[\\/|l*]'.format(eyes, nose), '<neutralface>', t)\n",
    "    t = re.sub(r'/', ' / ', t)\n",
    "    t = re.sub(r'<3','<heart>', t)\n",
    "    t = re.sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*', '<number>', t)\n",
    "    t = re.sub(r'#\\S+', hashtag, t)\n",
    "    t = re.sub(r'([!?.]){2,}', r'\\1 <repeat>', t)\n",
    "    t = re.sub(r'\\b(\\S*?)(.)\\2{2,}\\b', r'\\1\\2 <elong>', t)\n",
    "    t = re.sub(r'([A-Z]){2,}', allcaps, t)\n",
    "    t = re.sub(r'{}'.format(r'[\\\".,-;&:]'), ' ', t)\n",
    "    return t.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len: 134823\n",
      "Val data len: 5000\n",
      "Test data len: 10000\n"
     ]
    }
   ],
   "source": [
    "def get_data_list(path):\n",
    "    data = []\n",
    "    for id in open(data_dir + path, 'r').read().splitlines():\n",
    "\n",
    "        # process text (tweet special tokens)\n",
    "        text = tweet_dict[id]['tweet_text']\n",
    "        text = clean_tweet_text(text)\n",
    "\n",
    "        # get majority vote label\n",
    "        binary_labels = [1 if n > 0 else 0 for n in tweet_dict[id]['labels']]\n",
    "        label = 1 if sum(binary_labels)/len(tweet_dict[id]['labels']) > 0.5 else 0\n",
    "\n",
    "        # save to list\n",
    "        data.append((text, label))\n",
    "\n",
    "    return data\n",
    "    \n",
    "train_data = get_data_list('splits/train_ids.txt')\n",
    "val_data = get_data_list('splits/val_ids.txt')\n",
    "test_data = get_data_list('splits/test_ids.txt')\n",
    "print('Train data len:', len(train_data))\n",
    "print('Val data len:', len(val_data))\n",
    "print('Test data len:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thats  what u call a redneck lol <url> ['that', '##s', 'what', 'u', 'call', 'a', 'red', '##neck', 'lo', '##l', '<', 'ur', '##l', '>']\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer from bert library\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
    "print(train_data[0][0], tokenizer.tokenize(train_data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134823/134823 [00:33<00:00, 4083.11it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4105.61it/s]\n",
      "100%|██████████| 10000/10000 [00:02<00:00, 4132.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (134823, 50)\n",
      "Shape of label tensor: (134823,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sequences\n",
    "# https://colab.research.google.com/drive/1WQY_XxdiCVFzjMXnDdNfUjDFi0CN5hkT#scrollTo=TApTW_wLxoA9\n",
    "\n",
    "MAX_SEQ_LEN = 50\n",
    "\n",
    "def list_to_mats(l, tokenizer, pad_len):\n",
    "    texts, labels = zip(*l)\n",
    "    \n",
    "    sequences = []\n",
    "    for text in tqdm(texts):\n",
    "        tokens = [\"[CLS]\"] + tokenizer.tokenize(text) + [\"[SEP]\"]\n",
    "        tokens = [t for t in tokens if t[0]!='<'] # remove twitter specific text\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        sequences.append(token_ids)\n",
    "    \n",
    "    x = tf.keras.preprocessing.sequence.pad_sequences(sequences, pad_len, padding='post')\n",
    "    y = np.asarray(labels)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = list_to_mats(train_data, tokenizer, MAX_SEQ_LEN)\n",
    "x_val, y_val = list_to_mats(val_data, tokenizer, MAX_SEQ_LEN)\n",
    "x_test, y_test = list_to_mats(test_data, tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a distribution of predictions before training:\n",
    "\n",
    "preds = model.predict(x_test)\n",
    "%matplotlib inline\n",
    "plt.hist(preds)\n",
    "plt.xlim(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(1e-5), metrics=['accuracy'])\n",
    "\n",
    "# shuffle data\n",
    "indices = (np.arange(x_train.shape[0]))\n",
    "np.random.shuffle(indices)\n",
    "x_train, y_train = x_train[indices], y_train[indices]\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint(model_dir + 'best_text_bert.h5', \n",
    "                           save_best_only=True, \n",
    "                           monitor='val_loss', \n",
    "                           mode='min', \n",
    "                           save_weights_only=True)\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    epochs=5, \n",
    "                    batch_size=100, \n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[mcp_save])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_dir + 'best_text_bert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33\n"
     ]
    }
   ],
   "source": [
    "# pick best classification threshold from validation data\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "val_preds = model.predict(x_val)\n",
    "mx, thresh = 0, 0\n",
    "preds_bin = np.array(val_preds)\n",
    "\n",
    "for t in np.linspace(0, 1, 101):\n",
    "    preds_bin[val_preds >= t] = 1\n",
    "    preds_bin[val_preds < t] = 0\n",
    "    acc = accuracy_score(y_val, preds_bin)\n",
    "    if acc > mx: mx, thresh = acc, t\n",
    "        \n",
    "print(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.7400444799999999\n",
      "Test acc: 0.6859\n",
      "Test F1: 0.6801751349149782\n",
      "Test Precision: 0.6928023231694669\n",
      "Test Recall: 0.668\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# get AUROC\n",
    "preds = model.predict(x_test)\n",
    "print('Test AUROC:', roc_auc_score(y_test, preds))\n",
    "\n",
    "# get loss and acc\n",
    "preds_bin = np.array(preds)\n",
    "preds_bin[preds >= thresh] = 1\n",
    "preds_bin[preds < thresh] = 0\n",
    "print('Test acc:', accuracy_score(y_test, preds_bin))\n",
    "\n",
    "# get F1\n",
    "print('Test F1:', f1_score(y_test, preds_bin, zero_division=1))\n",
    "print('Test Precision:', precision_score(y_test, preds_bin, zero_division=1))\n",
    "print('Test Recall:', recall_score(y_test, preds_bin, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAUKklEQVR4nO3df5Bd5X3f8ffH4ofT2jFgNgyV5IomclPZncjMFsi40zrQgJA7Fpk6HjFNrDBMlabQcVpPGkj/wLFDB09r0zBjk8hFtfAkllUnKRqilKqAh3Gn/FgClhGEsgEcpMpoYwGJhzGtyLd/3EdHd/Cu9u7u1d2L/H7N3Nlzvuc55z7nmZU+e37ce1JVSJIE8Jbl7oAkaXwYCpKkjqEgSeoYCpKkjqEgSeqcttwdOJFzzz231qxZs9zdkKQ3lUcfffTPq2piMeuOdSisWbOGqamp5e6GJL2pJPnWYtf19JEkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqTPWn2g+Zs0NfzjS93v+lg+O9P0kaVx4pCBJ6hgKkqTOwKGQZEWSx5Lc3eYvSPJQkukkX0lyRquf2ean2/I1fdu4sdWfTnLFsHdGkrQ0CzlS+BjwVN/8p4Fbq+rHgJeAa1v9WuClVr+1tSPJOmAz8B5gA/D5JCuW1n1J0jANFApJVgEfBP5Tmw9wKfDV1mQHcFWb3tTmacsva+03ATur6rWqeg6YBi4axk5IkoZj0COF/wj8G+Cv2vw7gZer6mibPwCsbNMrgRcA2vJXWvuuPss6nSRbk0wlmZqZmVnArkiSlmreUEjyj4HDVfXoCPpDVW2rqsmqmpyYWNSDgyRJizTI5xTeD3woyUbgrcAPA78JnJXktHY0sAo42NofBFYDB5KcBrwD+E5f/Zj+dSRJY2DeI4WqurGqVlXVGnoXiu+rqn8K3A98uDXbAtzVpne3edry+6qqWn1zuzvpAmAt8PDQ9kSStGRL+UTzrwI7k/wG8BhwR6vfAXwpyTRwhF6QUFX7k+wCngSOAtdV1etLeH9J0pAtKBSq6mvA19r0s8xy91BVfQ/42TnWvxm4eaGdlCSNhp9oliR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUmfeUEjy1iQPJ/lGkv1Jfr3Vv5jkuSSPt9f6Vk+S25JMJ9mX5MK+bW1J8kx7bZnrPSVJy2OQJ6+9BlxaVd9Ncjrw9SR/1Jb9SlV99Q3tr6T3/OW1wMXA7cDFSc4BbgImgQIeTbK7ql4axo5IkpZu3iOF6vlumz29veoEq2wC7mzrPQicleR84Apgb1UdaUGwF9iwtO5LkoZpoGsKSVYkeRw4TO8/9ofaopvbKaJbk5zZaiuBF/pWP9Bqc9Xf+F5bk0wlmZqZmVng7kiSlmKgUKiq16tqPbAKuCjJe4EbgR8H/h5wDvCrw+hQVW2rqsmqmpyYmBjGJiVJA1rQ3UdV9TJwP7Chqg61U0SvAf8ZuKg1Owis7lttVavNVZckjYlB7j6aSHJWm/4h4KeBP2nXCUgS4CrgibbKbuCj7S6kS4BXquoQcA9weZKzk5wNXN5qkqQxMcjdR+cDO5KsoBciu6rq7iT3JZkAAjwO/PPWfg+wEZgGXgWuAaiqI0k+BTzS2n2yqo4Mb1ckSUs1byhU1T7gfbPUL52jfQHXzbFsO7B9gX2UJI2In2iWJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSZ5DHcb41ycNJvpFkf5Jfb/ULkjyUZDrJV5Kc0epntvnptnxN37ZubPWnk1xxsnZKkrQ4gxwpvAZcWlU/AawHNrRnL38auLWqfgx4Cbi2tb8WeKnVb23tSLIO2Ay8B9gAfL494lOSNCbmDYXq+W6bPb29CrgU+Gqr7wCuatOb2jxt+WVJ0uo7q+q1qnqO3jOcLxrKXkiShmKgawpJViR5HDgM7AX+FHi5qo62JgeAlW16JfACQFv+CvDO/vos6/S/19YkU0mmZmZmFr5HkqRFGygUqur1qloPrKL31/2Pn6wOVdW2qpqsqsmJiYmT9TaSpFks6O6jqnoZuB/4SeCsJKe1RauAg236ILAaoC1/B/Cd/vos60iSxsAgdx9NJDmrTf8Q8NPAU/TC4cOt2Rbgrja9u83Tlt9XVdXqm9vdSRcAa4GHh7UjkqSlO23+JpwP7Gh3Cr0F2FVVdyd5EtiZ5DeAx4A7Wvs7gC8lmQaO0LvjiKran2QX8CRwFLiuql4f7u5IkpZi3lCoqn3A+2apP8ssdw9V1feAn51jWzcDNy+8m5KkUfATzZKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoM8jjO1UnuT/Jkkv1JPtbqn0hyMMnj7bWxb50bk0wneTrJFX31Da02neSGk7NLkqTFGuRxnEeBj1fVHyd5O/Bokr1t2a1V9R/6GydZR+8RnO8B/gbwP5K8uy3+HL1nPB8AHkmyu6qeHMaOSJKWbpDHcR4CDrXpv0zyFLDyBKtsAnZW1WvAc+1Zzcce2zndHuNJkp2traEgSWNiQdcUkqyh97zmh1rp+iT7kmxPcnarrQRe6FvtQKvNVX/je2xNMpVkamZmZiHdkyQt0cChkORtwO8Bv1xVfwHcDvwosJ7ekcRnhtGhqtpWVZNVNTkxMTGMTUqSBjTINQWSnE4vEH6nqn4foKpe7Fv+BeDuNnsQWN23+qpW4wR1SdIYGOTuowB3AE9V1Wf76uf3NfsZ4Ik2vRvYnOTMJBcAa4GHgUeAtUkuSHIGvYvRu4ezG5KkYRjkSOH9wM8D30zyeKv9GnB1kvVAAc8DvwhQVfuT7KJ3AfkocF1VvQ6Q5HrgHmAFsL2q9g9xXyRJSzTI3UdfBzLLoj0nWOdm4OZZ6ntOtJ4kaXn5iWZJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1Bnkc5+ok9yd5Msn+JB9r9XOS7E3yTPt5dqsnyW1JppPsS3Jh37a2tPbPJNly8nZLkrQYgxwpHAU+XlXrgEuA65KsA24A7q2qtcC9bR7gSnrPZV4LbAVuh16IADcBFwMXATcdCxJJ0niYNxSq6lBV/XGb/kvgKWAlsAnY0ZrtAK5q05uAO6vnQeCsJOcDVwB7q+pIVb0E7AU2DHVvJElLsqBrCknWAO8DHgLOq6pDbdG3gfPa9Erghb7VDrTaXPU3vsfWJFNJpmZmZhbSPUnSEg0cCkneBvwe8MtV9Rf9y6qqgBpGh6pqW1VNVtXkxMTEMDYpSRrQQKGQ5HR6gfA7VfX7rfxiOy1E+3m41Q8Cq/tWX9Vqc9UlSWNikLuPAtwBPFVVn+1btBs4dgfRFuCuvvpH211IlwCvtNNM9wCXJzm7XWC+vNUkSWPitAHavB/4eeCbSR5vtV8DbgF2JbkW+BbwkbZsD7ARmAZeBa4BqKojST4FPNLafbKqjgxlLyRJQzFvKFTV14HMsfiyWdoXcN0c29oObF9IByVJo+MnmiVJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQZ5HGc25McTvJEX+0TSQ4meby9NvYtuzHJdJKnk1zRV9/QatNJbhj+rkiSlmqQI4UvAhtmqd9aVevbaw9AknXAZuA9bZ3PJ1mRZAXwOeBKYB1wdWsrSRojgzyO84Ekawbc3iZgZ1W9BjyXZBq4qC2brqpnAZLsbG2fXHCPJUknzVKuKVyfZF87vXR2q60EXuhrc6DV5qp/nyRbk0wlmZqZmVlC9yRJC7XYULgd+FFgPXAI+MywOlRV26pqsqomJyYmhrVZSdIA5j19NJuqevHYdJIvAHe32YPA6r6mq1qNE9QlSWNiUUcKSc7vm/0Z4NidSbuBzUnOTHIBsBZ4GHgEWJvkgiRn0LsYvXvx3ZYknQzzHikk+TLwAeDcJAeAm4APJFkPFPA88IsAVbU/yS56F5CPAtdV1ettO9cD9wArgO1VtX/oeyNJWpJB7j66epbyHSdofzNw8yz1PcCeBfVOkjRSfqJZktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktRZ1OM4T3VrbvjDkb/n87d8cOTvKUlvNO+RQpLtSQ4neaKvdk6SvUmeaT/PbvUkuS3JdJJ9SS7sW2dLa/9Mki0nZ3ckSUsxyOmjLwIb3lC7Abi3qtYC97Z5gCvpPZd5LbAVuB16IULvMZ4XAxcBNx0LEknS+Jg3FKrqAeDIG8qbgB1tegdwVV/9zup5EDgryfnAFcDeqjpSVS8Be/n+oJEkLbPFXmg+r6oOtelvA+e16ZXAC33tDrTaXHVJ0hhZ8t1HVVVADaEvACTZmmQqydTMzMywNitJGsBiQ+HFdlqI9vNwqx8EVve1W9Vqc9W/T1Vtq6rJqpqcmJhYZPckSYux2FtSdwNbgFvaz7v66tcn2UnvovIrVXUoyT3Av+u7uHw5cOPiu33qGfVtsN4CK2k284ZCki8DHwDOTXKA3l1EtwC7klwLfAv4SGu+B9gITAOvAtcAVNWRJJ8CHmntPllVb7x4LUlaZvOGQlVdPceiy2ZpW8B1c2xnO7B9Qb2TJI2UX3MhSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSer4kB3pTcwHQmnYPFKQJHUMBUlSx9NHkhbEL288tXmkIEnqeKQgDdFyXPiVhskjBUlSx1CQJHU8ffQDyvvbJc3GIwVJUmdJoZDk+STfTPJ4kqlWOyfJ3iTPtJ9nt3qS3JZkOsm+JBcOYwckScMzjCOFn6qq9VU12eZvAO6tqrXAvW0e4EpgbXttBW4fwntLkoboZJw+2gTsaNM7gKv66ndWz4PAWUnOPwnvL0lapKVeaC7gvycp4LerahtwXlUdasu/DZzXplcCL/Ste6DVDvXVSLKV3pEE73rXu5bYPf0g8zMD0sItNRT+flUdTPIjwN4kf9K/sKqqBcbAWrBsA5icnFzQuhpv/ictjb8lnT6qqoPt52HgD4CLgBePnRZqPw+35geB1X2rr2o1SdKYWHQoJPnrSd5+bBq4HHgC2A1sac22AHe16d3AR9tdSJcAr/SdZpIkjYGlnD46D/iDJMe287tV9d+SPALsSnIt8C3gI639HmAjMA28ClyzhPeWJJ0Eiw6FqnoW+IlZ6t8BLpulXsB1i30/SdLJ5yeaJUkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEkdQ0GS1DEUJEmdpT5PQZJOquV4Dsfzt3xw5O85LjxSkCR1DAVJUsdQkCR1DAVJUmfkoZBkQ5Knk0wnuWHU7y9JmttIQyHJCuBzwJXAOuDqJOtG2QdJ0txGfaRwETBdVc9W1f8FdgKbRtwHSdIcRv05hZXAC33zB4CL+xsk2QpsbbOvJXliRH0bd+cCf77cnRgTjsVxjsVxQxuLfHoYW1lWf3uxK47dh9eqahuwDSDJVFVNLnOXxoJjcZxjcZxjcZxjcVySqcWuO+rTRweB1X3zq1pNkjQGRh0KjwBrk1yQ5AxgM7B7xH2QJM1hpKePqupokuuBe4AVwPaq2n+CVbaNpmdvCo7FcY7FcY7FcY7FcYsei1TVMDsiSXoT8xPNkqSOoSBJ6oxFKMz31RdJzkzylbb8oSRrRt/L0RhgLP51kieT7Etyb5K/uRz9HIVBvxIlyT9JUklO2dsRBxmLJB9pvxv7k/zuqPs4KgP8G3lXkvuTPNb+nWxcjn6ebEm2Jzk812e50nNbG6d9SS4caMNVtawvehec/xT4W8AZwDeAdW9o8y+A32rTm4GvLHe/l3Esfgr4a236l36Qx6K1ezvwAPAgMLnc/V7G34u1wGPA2W3+R5a738s4FtuAX2rT64Dnl7vfJ2ks/gFwIfDEHMs3An8EBLgEeGiQ7Y7DkcIgX32xCdjRpr8KXJYkI+zjqMw7FlV1f1W92mYfpPdZj1PRoF+J8ing08D3Rtm5ERtkLP4Z8Lmqegmgqg6PuI+jMshYFPDDbfodwP8ZYf9GpqoeAI6coMkm4M7qeRA4K8n58213HEJhtq++WDlXm6o6CrwCvHMkvRutQcai37X0/hI4Fc07Fu1weHVVjf55jaM1yO/Fu4F3J/mfSR5MsmFkvRutQcbiE8DPJTkA7AH+5Wi6NnYW+v8JMIZfc6HBJPk5YBL4h8vdl+WQ5C3AZ4FfWOaujIvT6J1C+gC9o8cHkvzdqnp5WXu1PK4GvlhVn0nyk8CXkry3qv5quTv2ZjAORwqDfPVF1ybJafQOCb8zkt6N1kBfA5LkHwH/FvhQVb02or6N2nxj8XbgvcDXkjxP75zp7lP0YvMgvxcHgN1V9f+q6jngf9MLiVPNIGNxLbALoKr+F/BWel+W94NmUV8rNA6hMMhXX+wGtrTpDwP3VbuScoqZdyySvA/4bXqBcKqeN4Z5xqKqXqmqc6tqTVWtoXd95UNVtegvAhtjg/wb+a/0jhJIci6900nPjrKTIzLIWPwZcBlAkr9DLxRmRtrL8bAb+Gi7C+kS4JWqOjTfSst++qjm+OqLJJ8EpqpqN3AHvUPAaXoXVjYvX49PngHH4t8DbwP+S7vW/mdV9aFl6/RJMuBY/EAYcCzuAS5P8iTwOvArVXXKHU0POBYfB76Q5F/Ru+j8C6fiH5FJvkzvD4Fz2/WTm4DTAarqt+hdT9kITAOvAtcMtN1TcKwkSYs0DqePJEljwlCQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlS5/8DsFURS7xAj18AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(preds)\n",
    "plt.xlim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(model_dir + 'text_bert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  6343  6719  2025  1037  2309  3969  3449  5063  1028  2033  2613\n",
      "  4632  7743  2507  1037  6616  1005 10094  1037  9152 23033  9377  1028\n",
      " 24471  2140  1028   102     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0] 0\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36env",
   "language": "python",
   "name": "python36env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmf.common.registry import registry\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/awl27/mmf/mmf/utils/configuration.py:528: UserWarning: Device specified is 'cuda' but cuda is not present. Switching to CPU version.\n",
      "  + \"Switching to CPU version.\"\n",
      "/homes/awl27/python36env/lib/python3.6/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_cls = registry.get_model_class(\"visual_bert\")\n",
    "model = model_cls.from_pretrained(\"visual_bert.finetuned.hateful_memes.from_coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dir__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '_apply', '_get_name', '_load_from_state_dict', '_named_members', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', 'add_custom_params', 'add_module', 'apply', 'bfloat16', 'buffers', 'build', 'children', 'config_path', 'cpu', 'cuda', 'double', 'eval', 'extra_repr', 'flatten', 'flatten_for_bert', 'float', 'format_for_prediction', 'format_state_key', 'forward', 'from_pretrained', 'get_optimizer_parameters', 'half', 'init_losses', 'load_requirements', 'load_state_dict', 'model', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'share_memory', 'state_dict', 'to', 'train', 'type', 'update_sample_list_based_on_head', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "object_methods = [method_name for method_name in dir(model)\n",
    "                  if callable(getattr(model, method_name))]\n",
    "\n",
    "print(object_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "tokenizer = FullTokenizer(\n",
    "      vocab_file='pretrained_bert_model/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_dir = '../facebook_challenge_data/'\n",
    "model_dir = 'models/'\n",
    "\n",
    "# load data and print sizes\n",
    "# load data and print sizes\n",
    "def get_dict(path):\n",
    "    jsonl_content = open(path, 'r').read()\n",
    "    data = [json.loads(jline) for jline in jsonl_content.split('\\n')]\n",
    "    return {datum['id'] : datum for datum in data}\n",
    "\n",
    "\n",
    "train_dict = get_dict(data_dir + 'train.jsonl')\n",
    "val_dict = get_dict(data_dir + 'dev.jsonl')\n",
    "test_dict = get_dict(data_dir + 'test.jsonl')\n",
    "\n",
    "print(len(train_dict))\n",
    "print(len(val_dict))\n",
    "print(len(test_dict))\n",
    "\n",
    "def get_text_data(dictionary):\n",
    "    return [(datum['text'], datum['label']) for datum in dictionary.values()]\n",
    "\n",
    "train_data = get_text_data(train_dict)\n",
    "val_data = get_text_data(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8500/8500 [00:01<00:00, 4990.34it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 5791.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (8500, 50)\n",
      "Shape of label tensor: (8500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# tokenize the sequences\n",
    "# https://colab.research.google.com/drive/1WQY_XxdiCVFzjMXnDdNfUjDFi0CN5hkT#scrollTo=TApTW_wLxoA9\n",
    "\n",
    "MAX_SEQ_LEN = 50\n",
    "\n",
    "def list_to_mats(l, tokenizer, pad_len):\n",
    "    texts, labels = zip(*l)\n",
    "    \n",
    "    sequences = []\n",
    "    for text in tqdm(texts):\n",
    "        tokens = [\"[CLS]\"] + tokenizer.tokenize(text) + [\"[SEP]\"]\n",
    "        tokens = [t for t in tokens if t[0]!='<'] # remove twitter specific text\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        sequences.append(token_ids)\n",
    "    \n",
    "    x = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "    y = np.asarray(labels)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = list_to_mats(train_data, tokenizer, MAX_SEQ_LEN)\n",
    "x_val, y_val = list_to_mats(val_data, tokenizer, MAX_SEQ_LEN)\n",
    "# x_test, y_test = list_to_mats(test_data, tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use input mask and segment ids from\n",
    "# https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=IhJSe0QHNG7U\n",
    "from mmf.common.sample import Sample, SampleList\n",
    "\n",
    "input_mask = torch.tensor(np.ones(MAX_SEQ_LEN)).long()\n",
    "segment_ids = torch.tensor(np.zeros(MAX_SEQ_LEN)).long()\n",
    "\n",
    "sample_list = [Sample({'input_ids': torch.tensor(x).long(),\n",
    "                      'input_mask': input_mask,\n",
    "                      'segment_ids': segment_ids}) for x in x_val]\n",
    "sample_list = SampleList(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.94962453842163\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "preds = model.forward(sample_list)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': tensor([[ 3.3944, -3.3940],\n",
      "        [ 4.1372, -4.5100],\n",
      "        [ 4.0722, -4.3152],\n",
      "        [ 3.7807, -4.3434],\n",
      "        [ 4.1213, -4.4178],\n",
      "        [ 0.6177, -0.1645],\n",
      "        [ 3.9144, -4.2022],\n",
      "        [ 3.7445, -4.4305],\n",
      "        [ 3.9649, -4.4931],\n",
      "        [ 4.0661, -4.4082],\n",
      "        [ 4.0079, -4.7269],\n",
      "        [ 2.8871, -2.6498],\n",
      "        [ 3.8696, -4.3932],\n",
      "        [ 3.7421, -4.0838],\n",
      "        [ 3.7046, -4.1818],\n",
      "        [ 3.7688, -4.0062],\n",
      "        [ 3.9317, -4.4949],\n",
      "        [ 3.6897, -4.1206],\n",
      "        [ 3.6248, -3.9990],\n",
      "        [ 4.0347, -4.5704],\n",
      "        [ 4.0118, -4.5896],\n",
      "        [ 3.3246, -3.0516],\n",
      "        [ 3.6051, -3.9703],\n",
      "        [ 4.0415, -4.4295],\n",
      "        [ 3.9168, -4.4417],\n",
      "        [ 3.7617, -4.2302],\n",
      "        [ 3.6068, -4.1093],\n",
      "        [ 4.0320, -4.4828],\n",
      "        [ 3.5725, -3.8096],\n",
      "        [ 2.4044, -1.8997],\n",
      "        [ 3.2938, -3.7659],\n",
      "        [ 4.0818, -4.4964],\n",
      "        [ 3.9449, -4.2842],\n",
      "        [ 3.7612, -4.3850],\n",
      "        [ 3.7821, -3.8485],\n",
      "        [ 3.7872, -4.3553],\n",
      "        [ 3.8507, -4.3463],\n",
      "        [ 3.6090, -3.9877],\n",
      "        [ 3.9937, -4.3748],\n",
      "        [ 3.3075, -3.4363],\n",
      "        [ 3.9278, -4.1774],\n",
      "        [ 3.0854, -3.4375],\n",
      "        [ 4.1194, -4.4879],\n",
      "        [ 3.9880, -4.1696],\n",
      "        [ 3.7647, -4.3060],\n",
      "        [ 3.8640, -4.3121],\n",
      "        [ 3.9337, -4.1131],\n",
      "        [ 4.0138, -4.3794],\n",
      "        [ 4.0072, -4.6031],\n",
      "        [ 3.8032, -4.2012],\n",
      "        [ 4.0641, -4.5491],\n",
      "        [ 3.4174, -3.4413],\n",
      "        [ 3.9728, -4.4116],\n",
      "        [ 4.0632, -4.0697],\n",
      "        [ 3.5475, -3.9142],\n",
      "        [ 3.9823, -4.5020],\n",
      "        [ 3.7792, -4.2587],\n",
      "        [ 4.0178, -4.4229],\n",
      "        [ 3.4743, -3.6493],\n",
      "        [ 3.6556, -4.0970],\n",
      "        [ 4.0818, -4.4964],\n",
      "        [ 3.7126, -4.1400],\n",
      "        [ 4.0221, -4.4566],\n",
      "        [ 3.8999, -4.0447],\n",
      "        [ 3.3485, -3.3850],\n",
      "        [ 3.7221, -4.1966],\n",
      "        [ 3.9067, -4.0888],\n",
      "        [ 3.9823, -4.5020],\n",
      "        [ 4.0451, -4.4355],\n",
      "        [ 3.8344, -4.1222],\n",
      "        [ 3.6843, -3.8679],\n",
      "        [ 3.5426, -3.9734],\n",
      "        [ 4.0030, -4.1826],\n",
      "        [ 3.7202, -3.8985],\n",
      "        [ 3.7551, -4.3658],\n",
      "        [ 3.7542, -4.2427],\n",
      "        [ 4.0319, -4.1924],\n",
      "        [ 3.7412, -4.1428],\n",
      "        [ 3.6033, -3.9064],\n",
      "        [ 3.7960, -4.1612],\n",
      "        [ 3.8945, -4.5344],\n",
      "        [ 3.2211, -3.4275],\n",
      "        [ 3.9146, -4.2331],\n",
      "        [ 3.6207, -3.7174],\n",
      "        [ 3.7203, -4.1388],\n",
      "        [ 3.3638, -3.8492],\n",
      "        [ 4.0818, -4.4964],\n",
      "        [ 4.0477, -4.3500],\n",
      "        [ 3.1977, -3.5231],\n",
      "        [ 3.9637, -4.3681],\n",
      "        [ 3.7542, -4.2427],\n",
      "        [ 3.6142, -3.9042],\n",
      "        [ 3.0300, -2.7206],\n",
      "        [ 3.9326, -4.1910],\n",
      "        [ 3.8920, -3.9296],\n",
      "        [ 3.7420, -3.9610],\n",
      "        [ 3.3485, -3.3850],\n",
      "        [ 4.0692, -4.4245],\n",
      "        [ 4.1078, -4.5283],\n",
      "        [ 3.7020, -4.1525],\n",
      "        [ 4.0848, -4.5844],\n",
      "        [ 3.9178, -4.4499],\n",
      "        [ 3.9426, -4.2138],\n",
      "        [ 3.8374, -4.2853],\n",
      "        [ 3.7960, -4.1612],\n",
      "        [ 3.9168, -4.3423],\n",
      "        [ 3.5706, -4.1328],\n",
      "        [ 3.6207, -3.7174],\n",
      "        [ 4.1092, -4.5588],\n",
      "        [ 3.5776, -4.0197],\n",
      "        [ 3.7202, -3.8985],\n",
      "        [ 3.8203, -4.2219],\n",
      "        [ 3.2324, -3.4180],\n",
      "        [ 3.7182, -3.8364],\n",
      "        [ 3.9484, -4.4048],\n",
      "        [ 3.6781, -4.1101],\n",
      "        [ 3.9484, -4.3363],\n",
      "        [ 3.9911, -4.4043],\n",
      "        [ 3.6597, -3.9253],\n",
      "        [ 3.5827, -3.2161],\n",
      "        [ 4.1273, -4.4242],\n",
      "        [ 3.6541, -3.7975],\n",
      "        [ 3.9063, -3.8388],\n",
      "        [ 3.8016, -4.2711],\n",
      "        [ 3.9107, -4.3513],\n",
      "        [ 3.1927, -3.5119],\n",
      "        [ 3.9570, -4.3742],\n",
      "        [ 3.7842, -4.2765],\n",
      "        [ 3.9346, -4.3981],\n",
      "        [ 3.3331, -3.8353],\n",
      "        [ 3.5809, -3.7089],\n",
      "        [ 3.4726, -3.4497],\n",
      "        [ 3.8059, -4.1062],\n",
      "        [ 4.1334, -4.6062],\n",
      "        [ 3.8256, -4.1904],\n",
      "        [ 3.8814, -4.6119],\n",
      "        [ 3.9147, -4.3343],\n",
      "        [ 4.0808, -4.4247],\n",
      "        [ 4.1125, -4.4264],\n",
      "        [ 3.9420, -4.4198],\n",
      "        [ 3.9049, -4.2313],\n",
      "        [ 3.8574, -4.0751],\n",
      "        [ 4.0923, -4.5489],\n",
      "        [ 4.0109, -4.4503],\n",
      "        [ 3.7552, -4.1996],\n",
      "        [ 3.9260, -4.3885],\n",
      "        [ 2.1619, -2.0593],\n",
      "        [ 3.8648, -4.2803],\n",
      "        [ 3.6116, -4.1066],\n",
      "        [ 3.7800, -4.1269],\n",
      "        [ 3.9054, -4.3319],\n",
      "        [ 3.6469, -3.9875],\n",
      "        [ 3.6997, -4.0186],\n",
      "        [ 3.9426, -4.2138],\n",
      "        [ 4.0562, -4.4876],\n",
      "        [ 4.0239, -4.3472],\n",
      "        [ 3.9999, -4.6808],\n",
      "        [ 3.8374, -4.2853],\n",
      "        [ 3.7269, -3.8079],\n",
      "        [ 3.5572, -3.7921],\n",
      "        [ 4.0125, -4.3645],\n",
      "        [ 4.1165, -4.6330],\n",
      "        [ 3.3711, -3.6965],\n",
      "        [ 3.9063, -3.8388],\n",
      "        [ 3.9240, -4.1906],\n",
      "        [ 3.9284, -4.0302],\n",
      "        [ 3.2092, -3.5659],\n",
      "        [ 3.7541, -4.0403],\n",
      "        [ 3.9258, -4.1641],\n",
      "        [ 3.6886, -4.2696],\n",
      "        [ 3.7551, -4.0280],\n",
      "        [ 2.9179, -2.8649],\n",
      "        [ 3.9202, -4.3339],\n",
      "        [ 3.6512, -3.8861],\n",
      "        [ 3.6990, -3.9600],\n",
      "        [ 3.6530, -4.1834],\n",
      "        [ 4.0505, -4.4563],\n",
      "        [ 3.6158, -3.9365],\n",
      "        [ 4.1273, -4.4007],\n",
      "        [ 3.0620, -3.3417],\n",
      "        [ 3.8298, -4.5915],\n",
      "        [ 3.5845, -3.8362],\n",
      "        [ 3.5259, -3.5444],\n",
      "        [ 3.7096, -4.2216],\n",
      "        [ 4.0085, -4.3996],\n",
      "        [ 4.0363, -4.4817],\n",
      "        [ 4.1475, -4.5691],\n",
      "        [ 3.3941, -3.6123],\n",
      "        [ 4.1334, -4.6062],\n",
      "        [ 3.6118, -3.7778],\n",
      "        [ 3.5684, -3.6283],\n",
      "        [ 4.1585, -4.4199],\n",
      "        [ 4.0610, -4.4759],\n",
      "        [ 3.9054, -4.3319],\n",
      "        [ 3.9875, -4.4632],\n",
      "        [ 4.0470, -4.4295],\n",
      "        [ 3.8860, -4.2747],\n",
      "        [ 3.6387, -3.8494],\n",
      "        [ 3.4829, -3.9838],\n",
      "        [ 3.8554, -4.2262],\n",
      "        [ 4.0008, -4.2474],\n",
      "        [ 3.9567, -4.4516],\n",
      "        [ 4.0420, -4.4057],\n",
      "        [ 4.0030, -4.1826],\n",
      "        [ 3.5010, -3.5799],\n",
      "        [ 4.0286, -4.4317],\n",
      "        [ 3.8528, -4.2254],\n",
      "        [ 3.5527, -3.8277],\n",
      "        [ 2.4845, -2.3317],\n",
      "        [ 2.9799, -2.7124],\n",
      "        [ 3.8794, -4.2620],\n",
      "        [ 3.0116, -3.0350],\n",
      "        [ 3.6318, -3.7584],\n",
      "        [ 3.7742, -4.2047],\n",
      "        [ 3.5665, -3.5631],\n",
      "        [ 3.8668, -4.1910],\n",
      "        [ 3.6273, -3.7963],\n",
      "        [ 3.6912, -3.8458],\n",
      "        [ 3.9269, -4.1759],\n",
      "        [ 3.8596, -4.3153],\n",
      "        [ 3.9588, -4.3609],\n",
      "        [ 4.0135, -4.3166],\n",
      "        [ 3.7290, -3.7231],\n",
      "        [ 3.4835, -4.3195],\n",
      "        [ 3.9447, -4.3843],\n",
      "        [ 3.9056, -4.2441],\n",
      "        [ 4.1359, -4.4303],\n",
      "        [ 4.0284, -4.3515],\n",
      "        [ 3.9759, -4.4717],\n",
      "        [ 4.0933, -4.5122],\n",
      "        [ 3.8764, -4.1797],\n",
      "        [ 1.6454, -1.1095],\n",
      "        [ 4.1475, -4.5691],\n",
      "        [ 4.0085, -4.3996],\n",
      "        [ 3.9168, -4.3423],\n",
      "        [ 3.4355, -3.5087],\n",
      "        [ 3.5665, -3.5631],\n",
      "        [ 3.5195, -3.8129],\n",
      "        [ 3.7817, -4.2027],\n",
      "        [ 4.1077, -4.5260],\n",
      "        [ 3.9499, -4.4771],\n",
      "        [ 3.8897, -4.2956],\n",
      "        [ 3.2211, -3.4275],\n",
      "        [ 4.0797, -4.6363],\n",
      "        [ 3.7427, -4.1511],\n",
      "        [ 3.8498, -4.4691],\n",
      "        [ 3.8439, -4.2039],\n",
      "        [ 3.7423, -4.3848],\n",
      "        [ 3.7045, -4.1744],\n",
      "        [ 4.0332, -4.4732],\n",
      "        [ 3.8257, -4.0429],\n",
      "        [ 3.9180, -4.1377],\n",
      "        [ 3.9258, -4.1641],\n",
      "        [ 2.3488, -2.6397],\n",
      "        [ 4.1117, -4.6748],\n",
      "        [ 3.6581, -3.6234],\n",
      "        [ 3.4559, -3.9138],\n",
      "        [ 3.8085, -4.0493],\n",
      "        [ 3.9344, -4.3588],\n",
      "        [ 3.9655, -4.3050],\n",
      "        [ 2.3173, -1.9499],\n",
      "        [ 4.1168, -4.4155],\n",
      "        [ 3.5368, -3.8392],\n",
      "        [ 3.7795, -4.1326],\n",
      "        [ 3.9676, -4.4919],\n",
      "        [ 3.7610, -4.0667],\n",
      "        [ 3.8886, -4.2650],\n",
      "        [ 3.5809, -3.7089],\n",
      "        [ 3.8189, -4.2490],\n",
      "        [ 3.7269, -3.8079],\n",
      "        [ 4.0643, -4.2560],\n",
      "        [ 3.5399, -3.8045],\n",
      "        [ 4.0109, -4.4503],\n",
      "        [ 3.8229, -4.2309],\n",
      "        [ 3.8127, -3.9653],\n",
      "        [ 4.0923, -4.5489],\n",
      "        [ 3.9202, -4.3339],\n",
      "        [ 3.7697, -3.6376],\n",
      "        [ 3.9845, -4.3789],\n",
      "        [ 3.9899, -4.5251],\n",
      "        [ 3.7780, -4.0335],\n",
      "        [ 3.4541, -3.5323],\n",
      "        [ 3.9292, -4.3264],\n",
      "        [ 4.0066, -4.5203],\n",
      "        [ 3.7395, -4.2138],\n",
      "        [ 3.9513, -4.5852],\n",
      "        [ 4.1254, -4.4385],\n",
      "        [ 3.9238, -4.2681],\n",
      "        [ 3.9215, -4.4537],\n",
      "        [ 3.6228, -3.8261],\n",
      "        [ 4.0660, -4.3764],\n",
      "        [ 3.9528, -4.1982],\n",
      "        [ 3.9363, -4.4026],\n",
      "        [ 4.1090, -4.6118],\n",
      "        [ 3.5214, -3.7378],\n",
      "        [ 3.9834, -4.5114],\n",
      "        [ 4.0173, -4.3094],\n",
      "        [ 3.7013, -4.0177],\n",
      "        [ 3.9764, -4.7037],\n",
      "        [ 4.0173, -4.3094],\n",
      "        [ 3.8609, -4.3000],\n",
      "        [ 4.0472, -4.4376],\n",
      "        [ 3.7258, -4.0982],\n",
      "        [ 4.0045, -4.5526],\n",
      "        [ 4.0546, -4.6579],\n",
      "        [ 3.9266, -4.4782],\n",
      "        [ 3.7702, -4.3537],\n",
      "        [ 3.7236, -4.1273],\n",
      "        [ 4.0501, -4.5100],\n",
      "        [ 2.4603, -2.5477],\n",
      "        [ 1.6533, -1.7011],\n",
      "        [ 3.8953, -4.4196],\n",
      "        [ 3.9413, -4.3522],\n",
      "        [ 3.9974, -4.2223],\n",
      "        [ 4.0332, -4.4735],\n",
      "        [ 3.8899, -4.3642],\n",
      "        [ 3.5270, -3.8360],\n",
      "        [ 4.0763, -4.4417],\n",
      "        [ 1.4773, -1.0756],\n",
      "        [ 4.1010, -4.5664],\n",
      "        [ 3.6421, -3.9310],\n",
      "        [ 4.0954, -4.5022],\n",
      "        [ 3.9843, -4.4813],\n",
      "        [ 3.8965, -4.1427],\n",
      "        [ 3.3091, -3.4345],\n",
      "        [ 3.7778, -4.1882],\n",
      "        [ 3.5214, -3.7378],\n",
      "        [ 3.9104, -4.2612],\n",
      "        [ 3.9400, -4.4746],\n",
      "        [ 4.0308, -4.3459],\n",
      "        [ 3.9589, -4.3291],\n",
      "        [ 1.5784, -1.5418],\n",
      "        [ 3.9292, -4.3264],\n",
      "        [ 3.9176, -4.3800],\n",
      "        [ 3.5694, -3.9025],\n",
      "        [ 3.5252, -3.7627],\n",
      "        [ 3.7935, -3.7095],\n",
      "        [ 3.7930, -4.1224],\n",
      "        [ 3.8330, -4.2728],\n",
      "        [ 3.3834, -3.6371],\n",
      "        [ 4.0671, -4.5300],\n",
      "        [ 4.0908, -4.4490],\n",
      "        [ 2.6279, -2.7547],\n",
      "        [ 3.8953, -4.4196],\n",
      "        [ 4.0964, -4.0487],\n",
      "        [ 4.0003, -4.2640],\n",
      "        [ 4.0428, -4.4188],\n",
      "        [ 3.5514, -3.7424],\n",
      "        [ 3.7821, -4.0548],\n",
      "        [ 3.8834, -4.2803],\n",
      "        [ 3.5722, -3.8717],\n",
      "        [ 4.1561, -4.6527],\n",
      "        [ 3.5884, -3.6640],\n",
      "        [ 4.1193, -4.4090],\n",
      "        [ 4.1289, -4.5066],\n",
      "        [ 3.8097, -4.2020],\n",
      "        [ 3.7436, -3.9982],\n",
      "        [ 3.9283, -4.2299],\n",
      "        [ 3.7621, -4.0828],\n",
      "        [ 3.6909, -4.2324],\n",
      "        [ 3.6228, -3.8261],\n",
      "        [ 3.8943, -4.2064],\n",
      "        [ 2.4603, -2.5477],\n",
      "        [ 3.7988, -4.0418],\n",
      "        [ 3.7269, -3.9406],\n",
      "        [ 3.0703, -2.8306],\n",
      "        [ 3.9947, -4.6115],\n",
      "        [ 4.0264, -4.3818],\n",
      "        [ 4.0301, -4.5971],\n",
      "        [ 3.9884, -4.4459],\n",
      "        [ 3.8330, -4.2728],\n",
      "        [ 3.6421, -3.9310],\n",
      "        [ 3.3590, -3.6677],\n",
      "        [ 1.4773, -1.0756],\n",
      "        [ 3.7621, -4.0828],\n",
      "        [ 4.0938, -4.4305],\n",
      "        [ 4.0938, -4.4305],\n",
      "        [ 3.7782, -4.1500],\n",
      "        [ 2.2064, -2.0678],\n",
      "        [ 4.0098, -4.6671],\n",
      "        [ 3.8965, -4.1427],\n",
      "        [ 3.9589, -4.3291],\n",
      "        [ 4.0763, -4.4417],\n",
      "        [ 3.9160, -4.3477],\n",
      "        [ 3.7013, -4.0177],\n",
      "        [ 4.0065, -4.4657],\n",
      "        [ 3.8178, -4.2185],\n",
      "        [ 3.9528, -4.1982],\n",
      "        [ 3.5514, -3.7424],\n",
      "        [ 3.0294, -3.0467],\n",
      "        [ 3.9580, -4.2885],\n",
      "        [ 3.6771, -4.1105],\n",
      "        [ 4.0660, -4.3764],\n",
      "        [ 4.1008, -4.6168],\n",
      "        [ 3.9269, -4.0461],\n",
      "        [ 3.9539, -4.3141],\n",
      "        [ 3.8734, -4.3672],\n",
      "        [ 3.9842, -4.3299],\n",
      "        [ 3.8178, -4.2185],\n",
      "        [ 3.9283, -4.2299],\n",
      "        [ 3.9945, -4.4703],\n",
      "        [ 2.4714, -2.4234],\n",
      "        [ 4.0307, -4.5164],\n",
      "        [ 3.8697, -4.7190],\n",
      "        [ 3.7519, -3.7886],\n",
      "        [ 3.9069, -4.4099],\n",
      "        [ 4.0308, -4.3459],\n",
      "        [ 3.3590, -3.6677],\n",
      "        [ 3.4652, -3.7981],\n",
      "        [ 4.0883, -4.5818],\n",
      "        [ 4.0165, -4.3667],\n",
      "        [ 3.7930, -4.1224],\n",
      "        [ 4.1475, -4.5691],\n",
      "        [ 3.6420, -3.7512],\n",
      "        [ 3.9539, -4.3141],\n",
      "        [ 3.9986, -4.4680],\n",
      "        [ 3.5270, -3.8360],\n",
      "        [ 4.0826, -4.5639],\n",
      "        [ 3.9162, -4.3021],\n",
      "        [ 4.1193, -4.4090],\n",
      "        [ 3.5704, -3.4277],\n",
      "        [ 3.5150, -3.5243],\n",
      "        [ 3.3252, -3.3860],\n",
      "        [ 3.7436, -3.9982],\n",
      "        [ 4.0189, -4.7062],\n",
      "        [ 3.4995, -3.9820],\n",
      "        [ 3.9131, -4.5115],\n",
      "        [ 2.3986, -2.5261],\n",
      "        [ 4.1482, -4.5057],\n",
      "        [ 3.7112, -3.5577],\n",
      "        [ 3.7941, -4.2215],\n",
      "        [ 3.7269, -3.9406],\n",
      "        [ 3.8043, -4.2820],\n",
      "        [ 3.8121, -4.1727],\n",
      "        [ 3.8734, -4.3672],\n",
      "        [ 4.0301, -4.5971],\n",
      "        [ 2.0161, -1.4441],\n",
      "        [ 3.9692, -4.5872],\n",
      "        [ 3.4861, -3.7334],\n",
      "        [ 4.1010, -4.5664],\n",
      "        [ 3.9044, -4.2556],\n",
      "        [ 3.9382, -4.2217],\n",
      "        [ 2.5122, -2.2367],\n",
      "        [ 3.5150, -3.5243],\n",
      "        [ 3.5722, -3.8717],\n",
      "        [ 3.9996, -4.1543],\n",
      "        [ 3.8609, -4.3000],\n",
      "        [ 3.9186, -4.1161],\n",
      "        [ 3.6251, -3.8062],\n",
      "        [ 3.9843, -4.4813],\n",
      "        [ 2.3986, -2.5261],\n",
      "        [ 3.6747, -4.2406],\n",
      "        [ 4.0844, -4.4499],\n",
      "        [ 3.9014, -4.4275],\n",
      "        [ 3.9104, -4.2612],\n",
      "        [ 4.0066, -4.5203],\n",
      "        [ 4.0656, -4.6022],\n",
      "        [ 3.9914, -4.5831],\n",
      "        [ 4.0207, -4.3625],\n",
      "        [ 3.3834, -3.6371],\n",
      "        [ 4.0050, -4.6638],\n",
      "        [ 4.0601, -4.4547],\n",
      "        [ 3.7236, -4.1273],\n",
      "        [ 3.2456, -3.3123],\n",
      "        [ 3.7935, -3.7095],\n",
      "        [ 3.9692, -4.5872],\n",
      "        [ 3.7723, -3.9260],\n",
      "        [ 3.6022, -4.0574],\n",
      "        [ 3.9842, -4.3299],\n",
      "        [ 3.9996, -4.1543],\n",
      "        [ 4.1475, -4.5691],\n",
      "        [ 3.8151, -4.2021],\n",
      "        [ 3.7914, -4.2641],\n",
      "        [ 3.9652, -4.3448],\n",
      "        [ 3.9811, -4.4954],\n",
      "        [ 3.7112, -3.5577],\n",
      "        [ 4.0122, -4.1112],\n",
      "        [ 3.9069, -4.4099],\n",
      "        [ 4.0770, -4.5100],\n",
      "        [ 4.1080, -4.5193],\n",
      "        [ 3.3091, -3.4345],\n",
      "        [ 4.0025, -4.5350],\n",
      "        [ 3.7790, -4.2251],\n",
      "        [ 4.1052, -4.4610],\n",
      "        [ 4.0844, -4.4499],\n",
      "        [ 3.8811, -4.4567],\n",
      "        [ 3.9036, -4.4058],\n",
      "        [ 4.0501, -4.5100],\n",
      "        [ 4.0122, -4.1112],\n",
      "        [ 3.8043, -4.2820],\n",
      "        [ 3.8811, -4.4567],\n",
      "        [ 3.9382, -4.2217],\n",
      "        [ 4.1042, -4.5541],\n",
      "        [ 3.9363, -4.4026],\n",
      "        [ 3.9226, -4.3052],\n",
      "        [ 3.8986, -4.4202],\n",
      "        [ 3.9809, -4.4360],\n",
      "        [ 3.9186, -4.1161],\n",
      "        [ 3.7988, -3.4905],\n",
      "        [ 3.4057, -3.2936]], grad_fn=<ViewBackward>)}\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "pred_list = preds['scores'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9887e-01, 1.1255e-03],\n",
      "        [9.9982e-01, 1.7560e-04],\n",
      "        [9.9977e-01, 2.2767e-04],\n",
      "        [9.9970e-01, 2.9622e-04],\n",
      "        [9.9980e-01, 1.9563e-04],\n",
      "        [6.8615e-01, 3.1385e-01],\n",
      "        [9.9970e-01, 2.9847e-04],\n",
      "        [9.9972e-01, 2.8152e-04],\n",
      "        [9.9979e-01, 2.1215e-04],\n",
      "        [9.9979e-01, 2.0871e-04],\n",
      "        [9.9984e-01, 1.6086e-04],\n",
      "        [9.9608e-01, 3.9233e-03],\n",
      "        [9.9974e-01, 2.5785e-04],\n",
      "        [9.9960e-01, 3.9909e-04],\n",
      "        [9.9962e-01, 3.7569e-04],\n",
      "        [9.9958e-01, 4.1991e-04],\n",
      "        [9.9978e-01, 2.1892e-04],\n",
      "        [9.9959e-01, 4.0538e-04],\n",
      "        [9.9951e-01, 4.8843e-04],\n",
      "        [9.9982e-01, 1.8312e-04],\n",
      "        [9.9982e-01, 1.8380e-04],\n",
      "        [9.9830e-01, 1.6986e-03],\n",
      "        [9.9949e-01, 5.1263e-04],\n",
      "        [9.9979e-01, 2.0940e-04],\n",
      "        [9.9977e-01, 2.3434e-04],\n",
      "        [9.9966e-01, 3.3807e-04],\n",
      "        [9.9955e-01, 4.4542e-04],\n",
      "        [9.9980e-01, 2.0044e-04],\n",
      "        [9.9938e-01, 6.2194e-04],\n",
      "        [9.8667e-01, 1.3333e-02],\n",
      "        [9.9914e-01, 8.5826e-04],\n",
      "        [9.9981e-01, 1.8813e-04],\n",
      "        [9.9973e-01, 2.6668e-04],\n",
      "        [9.9971e-01, 2.8973e-04],\n",
      "        [9.9951e-01, 4.8513e-04],\n",
      "        [9.9971e-01, 2.9084e-04],\n",
      "        [9.9972e-01, 2.7539e-04],\n",
      "        [9.9950e-01, 5.0185e-04],\n",
      "        [9.9977e-01, 2.3203e-04],\n",
      "        [9.9882e-01, 1.1768e-03],\n",
      "        [9.9970e-01, 3.0185e-04],\n",
      "        [9.9853e-01, 1.4673e-03],\n",
      "        [9.9982e-01, 1.8275e-04],\n",
      "        [9.9971e-01, 2.8648e-04],\n",
      "        [9.9969e-01, 3.1248e-04],\n",
      "        [9.9972e-01, 2.8120e-04],\n",
      "        [9.9968e-01, 3.2003e-04],\n",
      "        [9.9977e-01, 2.2635e-04],\n",
      "        [9.9982e-01, 1.8219e-04],\n",
      "        [9.9967e-01, 3.3388e-04],\n",
      "        [9.9982e-01, 1.8166e-04],\n",
      "        [9.9895e-01, 1.0492e-03],\n",
      "        [9.9977e-01, 2.2835e-04],\n",
      "        [9.9971e-01, 2.9363e-04],\n",
      "        [9.9943e-01, 5.7435e-04],\n",
      "        [9.9979e-01, 2.0663e-04],\n",
      "        [9.9968e-01, 3.2288e-04],\n",
      "        [9.9978e-01, 2.1585e-04],\n",
      "        [9.9919e-01, 8.0518e-04],\n",
      "        [9.9957e-01, 4.2942e-04],\n",
      "        [9.9981e-01, 1.8813e-04],\n",
      "        [9.9961e-01, 3.8862e-04],\n",
      "        [9.9979e-01, 2.0780e-04],\n",
      "        [9.9965e-01, 3.5445e-04],\n",
      "        [9.9881e-01, 1.1890e-03],\n",
      "        [9.9964e-01, 3.6375e-04],\n",
      "        [9.9966e-01, 3.3686e-04],\n",
      "        [9.9979e-01, 2.0663e-04],\n",
      "        [9.9979e-01, 2.0740e-04],\n",
      "        [9.9965e-01, 3.5022e-04],\n",
      "        [9.9948e-01, 5.2466e-04],\n",
      "        [9.9946e-01, 5.4400e-04],\n",
      "        [9.9972e-01, 2.7857e-04],\n",
      "        [9.9951e-01, 4.9093e-04],\n",
      "        [9.9970e-01, 2.9716e-04],\n",
      "        [9.9966e-01, 3.3639e-04],\n",
      "        [9.9973e-01, 2.6801e-04],\n",
      "        [9.9962e-01, 3.7659e-04],\n",
      "        [9.9945e-01, 5.4746e-04],\n",
      "        [9.9965e-01, 3.5003e-04],\n",
      "        [9.9978e-01, 2.1840e-04],\n",
      "        [9.9871e-01, 1.2941e-03],\n",
      "        [9.9971e-01, 2.8933e-04],\n",
      "        [9.9935e-01, 6.4981e-04],\n",
      "        [9.9961e-01, 3.8609e-04],\n",
      "        [9.9926e-01, 7.3636e-04],\n",
      "        [9.9981e-01, 1.8813e-04],\n",
      "        [9.9977e-01, 2.2534e-04],\n",
      "        [9.9880e-01, 1.2042e-03],\n",
      "        [9.9976e-01, 2.4068e-04],\n",
      "        [9.9966e-01, 3.3639e-04],\n",
      "        [9.9946e-01, 5.4269e-04],\n",
      "        [9.9683e-01, 3.1709e-03],\n",
      "        [9.9970e-01, 2.9638e-04],\n",
      "        [9.9960e-01, 4.0081e-04],\n",
      "        [9.9955e-01, 4.5127e-04],\n",
      "        [9.9881e-01, 1.1890e-03],\n",
      "        [9.9980e-01, 2.0471e-04],\n",
      "        [9.9982e-01, 1.7753e-04],\n",
      "        [9.9961e-01, 3.8785e-04],\n",
      "        [9.9983e-01, 1.7178e-04],\n",
      "        [9.9977e-01, 2.3219e-04],\n",
      "        [9.9971e-01, 2.8682e-04],\n",
      "        [9.9970e-01, 2.9662e-04],\n",
      "        [9.9965e-01, 3.5003e-04],\n",
      "        [9.9974e-01, 2.5883e-04],\n",
      "        [9.9955e-01, 4.5110e-04],\n",
      "        [9.9935e-01, 6.4981e-04],\n",
      "        [9.9983e-01, 1.7199e-04],\n",
      "        [9.9950e-01, 5.0158e-04],\n",
      "        [9.9951e-01, 4.9093e-04],\n",
      "        [9.9968e-01, 3.2149e-04],\n",
      "        [9.9871e-01, 1.2918e-03],\n",
      "        [9.9948e-01, 5.2340e-04],\n",
      "        [9.9976e-01, 2.3558e-04],\n",
      "        [9.9959e-01, 4.1442e-04],\n",
      "        [9.9975e-01, 2.5227e-04],\n",
      "        [9.9977e-01, 2.2586e-04],\n",
      "        [9.9949e-01, 5.0774e-04],\n",
      "        [9.9889e-01, 1.1138e-03],\n",
      "        [9.9981e-01, 1.9321e-04],\n",
      "        [9.9942e-01, 5.8015e-04],\n",
      "        [9.9957e-01, 4.3267e-04],\n",
      "        [9.9969e-01, 3.1183e-04],\n",
      "        [9.9974e-01, 2.5808e-04],\n",
      "        [9.9878e-01, 1.2238e-03],\n",
      "        [9.9976e-01, 2.4083e-04],\n",
      "        [9.9968e-01, 3.1562e-04],\n",
      "        [9.9976e-01, 2.4047e-04],\n",
      "        [9.9923e-01, 7.6998e-04],\n",
      "        [9.9932e-01, 6.8197e-04],\n",
      "        [9.9902e-01, 9.8453e-04],\n",
      "        [9.9963e-01, 3.6616e-04],\n",
      "        [9.9984e-01, 1.6009e-04],\n",
      "        [9.9967e-01, 3.3003e-04],\n",
      "        [9.9980e-01, 2.0480e-04],\n",
      "        [9.9974e-01, 2.6146e-04],\n",
      "        [9.9980e-01, 2.0231e-04],\n",
      "        [9.9980e-01, 1.9568e-04],\n",
      "        [9.9977e-01, 2.3357e-04],\n",
      "        [9.9971e-01, 2.9266e-04],\n",
      "        [9.9964e-01, 3.5874e-04],\n",
      "        [9.9982e-01, 1.7664e-04],\n",
      "        [9.9979e-01, 2.1149e-04],\n",
      "        [9.9965e-01, 3.5086e-04],\n",
      "        [9.9976e-01, 2.4488e-04],\n",
      "        [9.8553e-01, 1.4469e-02],\n",
      "        [9.9971e-01, 2.9008e-04],\n",
      "        [9.9956e-01, 4.4447e-04],\n",
      "        [9.9963e-01, 3.6807e-04],\n",
      "        [9.9974e-01, 2.6452e-04],\n",
      "        [9.9952e-01, 4.8330e-04],\n",
      "        [9.9956e-01, 4.4443e-04],\n",
      "        [9.9971e-01, 2.8682e-04],\n",
      "        [9.9981e-01, 1.9472e-04],\n",
      "        [9.9977e-01, 2.3141e-04],\n",
      "        [9.9983e-01, 1.6980e-04],\n",
      "        [9.9970e-01, 2.9662e-04],\n",
      "        [9.9947e-01, 5.3387e-04],\n",
      "        [9.9936e-01, 6.4266e-04],\n",
      "        [9.9977e-01, 2.3004e-04],\n",
      "        [9.9984e-01, 1.5851e-04],\n",
      "        [9.9915e-01, 8.5154e-04],\n",
      "        [9.9957e-01, 4.3267e-04],\n",
      "        [9.9970e-01, 2.9904e-04],\n",
      "        [9.9965e-01, 3.4954e-04],\n",
      "        [9.9886e-01, 1.1405e-03],\n",
      "        [9.9959e-01, 4.1187e-04],\n",
      "        [9.9969e-01, 3.0653e-04],\n",
      "        [9.9965e-01, 3.4968e-04],\n",
      "        [9.9958e-01, 4.1655e-04],\n",
      "        [9.9693e-01, 3.0706e-03],\n",
      "        [9.9974e-01, 2.6012e-04],\n",
      "        [9.9947e-01, 5.3257e-04],\n",
      "        [9.9953e-01, 4.7158e-04],\n",
      "        [9.9961e-01, 3.9493e-04],\n",
      "        [9.9980e-01, 2.0204e-04],\n",
      "        [9.9948e-01, 5.2462e-04],\n",
      "        [9.9980e-01, 1.9781e-04],\n",
      "        [9.9835e-01, 1.6526e-03],\n",
      "        [9.9978e-01, 2.2008e-04],\n",
      "        [9.9940e-01, 5.9838e-04],\n",
      "        [9.9915e-01, 8.4927e-04],\n",
      "        [9.9964e-01, 3.5921e-04],\n",
      "        [9.9978e-01, 2.2302e-04],\n",
      "        [9.9980e-01, 1.9981e-04],\n",
      "        [9.9984e-01, 1.6381e-04],\n",
      "        [9.9909e-01, 9.0524e-04],\n",
      "        [9.9984e-01, 1.6009e-04],\n",
      "        [9.9938e-01, 6.1727e-04],\n",
      "        [9.9925e-01, 7.4848e-04],\n",
      "        [9.9981e-01, 1.8809e-04],\n",
      "        [9.9980e-01, 1.9607e-04],\n",
      "        [9.9974e-01, 2.6452e-04],\n",
      "        [9.9979e-01, 2.1372e-04],\n",
      "        [9.9979e-01, 2.0824e-04],\n",
      "        [9.9971e-01, 2.8556e-04],\n",
      "        [9.9944e-01, 5.5939e-04],\n",
      "        [9.9943e-01, 5.7152e-04],\n",
      "        [9.9969e-01, 3.0910e-04],\n",
      "        [9.9974e-01, 2.6166e-04],\n",
      "        [9.9978e-01, 2.2295e-04],\n",
      "        [9.9979e-01, 2.1434e-04],\n",
      "        [9.9972e-01, 2.7857e-04],\n",
      "        [9.9916e-01, 8.4036e-04],\n",
      "        [9.9979e-01, 2.1164e-04],\n",
      "        [9.9969e-01, 3.1016e-04],\n",
      "        [9.9938e-01, 6.2298e-04],\n",
      "        [9.9197e-01, 8.0320e-03],\n",
      "        [9.9664e-01, 3.3604e-03],\n",
      "        [9.9971e-01, 2.9115e-04],\n",
      "        [9.9764e-01, 2.3603e-03],\n",
      "        [9.9938e-01, 6.1686e-04],\n",
      "        [9.9966e-01, 3.4251e-04],\n",
      "        [9.9920e-01, 8.0034e-04],\n",
      "        [9.9968e-01, 3.1652e-04],\n",
      "        [9.9940e-01, 5.9667e-04],\n",
      "        [9.9947e-01, 5.3271e-04],\n",
      "        [9.9970e-01, 3.0259e-04],\n",
      "        [9.9972e-01, 2.8156e-04],\n",
      "        [9.9976e-01, 2.4360e-04],\n",
      "        [9.9976e-01, 2.4108e-04],\n",
      "        [9.9942e-01, 5.7990e-04],\n",
      "        [9.9959e-01, 4.0833e-04],\n",
      "        [9.9976e-01, 2.4135e-04],\n",
      "        [9.9971e-01, 2.8873e-04],\n",
      "        [9.9981e-01, 1.9039e-04],\n",
      "        [9.9977e-01, 2.2938e-04],\n",
      "        [9.9979e-01, 2.1435e-04],\n",
      "        [9.9982e-01, 1.8307e-04],\n",
      "        [9.9968e-01, 3.1708e-04],\n",
      "        [9.4019e-01, 5.9812e-02],\n",
      "        [9.9984e-01, 1.6381e-04],\n",
      "        [9.9978e-01, 2.2302e-04],\n",
      "        [9.9974e-01, 2.5883e-04],\n",
      "        [9.9904e-01, 9.6330e-04],\n",
      "        [9.9920e-01, 8.0034e-04],\n",
      "        [9.9935e-01, 6.5358e-04],\n",
      "        [9.9966e-01, 3.4064e-04],\n",
      "        [9.9982e-01, 1.7797e-04],\n",
      "        [9.9978e-01, 2.1882e-04],\n",
      "        [9.9972e-01, 2.7862e-04],\n",
      "        [9.9871e-01, 1.2941e-03],\n",
      "        [9.9984e-01, 1.6391e-04],\n",
      "        [9.9963e-01, 3.7290e-04],\n",
      "        [9.9976e-01, 2.4382e-04],\n",
      "        [9.9968e-01, 3.1971e-04],\n",
      "        [9.9970e-01, 2.9535e-04],\n",
      "        [9.9962e-01, 3.7850e-04],\n",
      "        [9.9980e-01, 2.0212e-04],\n",
      "        [9.9962e-01, 3.8241e-04],\n",
      "        [9.9968e-01, 3.1719e-04],\n",
      "        [9.9969e-01, 3.0653e-04],\n",
      "        [9.9323e-01, 6.7700e-03],\n",
      "        [9.9985e-01, 1.5277e-04],\n",
      "        [9.9931e-01, 6.8770e-04],\n",
      "        [9.9937e-01, 6.2967e-04],\n",
      "        [9.9961e-01, 3.8660e-04],\n",
      "        [9.9975e-01, 2.5014e-04],\n",
      "        [9.9974e-01, 2.5588e-04],\n",
      "        [9.8617e-01, 1.3828e-02],\n",
      "        [9.9980e-01, 1.9697e-04],\n",
      "        [9.9937e-01, 6.2571e-04],\n",
      "        [9.9963e-01, 3.6614e-04],\n",
      "        [9.9979e-01, 2.1184e-04],\n",
      "        [9.9960e-01, 3.9837e-04],\n",
      "        [9.9971e-01, 2.8763e-04],\n",
      "        [9.9932e-01, 6.8197e-04],\n",
      "        [9.9969e-01, 3.1335e-04],\n",
      "        [9.9947e-01, 5.3387e-04],\n",
      "        [9.9976e-01, 2.4347e-04],\n",
      "        [9.9935e-01, 6.4577e-04],\n",
      "        [9.9979e-01, 2.1149e-04],\n",
      "        [9.9968e-01, 3.1781e-04],\n",
      "        [9.9958e-01, 4.1869e-04],\n",
      "        [9.9982e-01, 1.7664e-04],\n",
      "        [9.9974e-01, 2.6012e-04],\n",
      "        [9.9939e-01, 6.0646e-04],\n",
      "        [9.9977e-01, 2.3320e-04],\n",
      "        [9.9980e-01, 2.0040e-04],\n",
      "        [9.9960e-01, 4.0487e-04],\n",
      "        [9.9908e-01, 9.2348e-04],\n",
      "        [9.9974e-01, 2.5973e-04],\n",
      "        [9.9980e-01, 1.9803e-04],\n",
      "        [9.9965e-01, 3.5139e-04],\n",
      "        [9.9980e-01, 1.9615e-04],\n",
      "        [9.9981e-01, 1.9083e-04],\n",
      "        [9.9972e-01, 2.7683e-04],\n",
      "        [9.9977e-01, 2.3045e-04],\n",
      "        [9.9942e-01, 5.8179e-04],\n",
      "        [9.9978e-01, 2.1548e-04],\n",
      "        [9.9971e-01, 2.8838e-04],\n",
      "        [9.9976e-01, 2.3899e-04],\n",
      "        [9.9984e-01, 1.6312e-04],\n",
      "        [9.9930e-01, 7.0319e-04],\n",
      "        [9.9980e-01, 2.0450e-04],\n",
      "        [9.9976e-01, 2.4193e-04],\n",
      "        [9.9956e-01, 4.4412e-04],\n",
      "        [9.9983e-01, 1.6991e-04],\n",
      "        [9.9976e-01, 2.4193e-04],\n",
      "        [9.9971e-01, 2.8552e-04],\n",
      "        [9.9979e-01, 2.0654e-04],\n",
      "        [9.9960e-01, 3.9985e-04],\n",
      "        [9.9981e-01, 1.9212e-04],\n",
      "        [9.9984e-01, 1.6449e-04],\n",
      "        [9.9978e-01, 2.2373e-04],\n",
      "        [9.9970e-01, 2.9628e-04],\n",
      "        [9.9961e-01, 3.8928e-04],\n",
      "        [9.9981e-01, 1.9155e-04],\n",
      "        [9.9336e-01, 6.6393e-03],\n",
      "        [9.6625e-01, 3.3751e-02],\n",
      "        [9.9976e-01, 2.4479e-04],\n",
      "        [9.9975e-01, 2.5008e-04],\n",
      "        [9.9973e-01, 2.6922e-04],\n",
      "        [9.9980e-01, 2.0206e-04],\n",
      "        [9.9974e-01, 2.6011e-04],\n",
      "        [9.9937e-01, 6.3387e-04],\n",
      "        [9.9980e-01, 1.9980e-04],\n",
      "        [9.2777e-01, 7.2234e-02],\n",
      "        [9.9983e-01, 1.7208e-04],\n",
      "        [9.9949e-01, 5.1384e-04],\n",
      "        [9.9982e-01, 1.8450e-04],\n",
      "        [9.9979e-01, 2.1055e-04],\n",
      "        [9.9968e-01, 3.2246e-04],\n",
      "        [9.9882e-01, 1.1770e-03],\n",
      "        [9.9965e-01, 3.4693e-04],\n",
      "        [9.9930e-01, 7.0319e-04],\n",
      "        [9.9972e-01, 2.8247e-04],\n",
      "        [9.9978e-01, 2.2155e-04],\n",
      "        [9.9977e-01, 2.3011e-04],\n",
      "        [9.9975e-01, 2.5145e-04],\n",
      "        [9.5772e-01, 4.2282e-02],\n",
      "        [9.9974e-01, 2.5973e-04],\n",
      "        [9.9975e-01, 2.4904e-04],\n",
      "        [9.9943e-01, 5.6853e-04],\n",
      "        [9.9932e-01, 6.8328e-04],\n",
      "        [9.9945e-01, 5.5112e-04],\n",
      "        [9.9963e-01, 3.6495e-04],\n",
      "        [9.9970e-01, 3.0168e-04],\n",
      "        [9.9911e-01, 8.9252e-04],\n",
      "        [9.9982e-01, 1.8462e-04],\n",
      "        [9.9980e-01, 1.9548e-04],\n",
      "        [9.9543e-01, 4.5748e-03],\n",
      "        [9.9976e-01, 2.4479e-04],\n",
      "        [9.9971e-01, 2.9007e-04],\n",
      "        [9.9974e-01, 2.5748e-04],\n",
      "        [9.9979e-01, 2.1138e-04],\n",
      "        [9.9932e-01, 6.7927e-04],\n",
      "        [9.9961e-01, 3.9474e-04],\n",
      "        [9.9972e-01, 2.8473e-04],\n",
      "        [9.9942e-01, 5.8465e-04],\n",
      "        [9.9985e-01, 1.4939e-04],\n",
      "        [9.9929e-01, 7.0797e-04],\n",
      "        [9.9980e-01, 1.9775e-04],\n",
      "        [9.9982e-01, 1.7765e-04],\n",
      "        [9.9967e-01, 3.3147e-04],\n",
      "        [9.9957e-01, 4.3408e-04],\n",
      "        [9.9971e-01, 2.8628e-04],\n",
      "        [9.9961e-01, 3.9158e-04],\n",
      "        [9.9964e-01, 3.6206e-04],\n",
      "        [9.9942e-01, 5.8179e-04],\n",
      "        [9.9970e-01, 3.0321e-04],\n",
      "        [9.9336e-01, 6.6393e-03],\n",
      "        [9.9961e-01, 3.9328e-04],\n",
      "        [9.9953e-01, 4.6758e-04],\n",
      "        [9.9727e-01, 2.7294e-03],\n",
      "        [9.9982e-01, 1.8294e-04],\n",
      "        [9.9978e-01, 2.2298e-04],\n",
      "        [9.9982e-01, 1.7914e-04],\n",
      "        [9.9978e-01, 2.1724e-04],\n",
      "        [9.9970e-01, 3.0168e-04],\n",
      "        [9.9949e-01, 5.1384e-04],\n",
      "        [9.9911e-01, 8.8701e-04],\n",
      "        [9.2777e-01, 7.2234e-02],\n",
      "        [9.9961e-01, 3.9158e-04],\n",
      "        [9.9980e-01, 1.9854e-04],\n",
      "        [9.9980e-01, 1.9854e-04],\n",
      "        [9.9964e-01, 3.6032e-04],\n",
      "        [9.8627e-01, 1.3731e-02],\n",
      "        [9.9983e-01, 1.7046e-04],\n",
      "        [9.9968e-01, 3.2246e-04],\n",
      "        [9.9975e-01, 2.5145e-04],\n",
      "        [9.9980e-01, 1.9980e-04],\n",
      "        [9.9974e-01, 2.5764e-04],\n",
      "        [9.9956e-01, 4.4412e-04],\n",
      "        [9.9979e-01, 2.0917e-04],\n",
      "        [9.9968e-01, 3.2339e-04],\n",
      "        [9.9971e-01, 2.8838e-04],\n",
      "        [9.9932e-01, 6.7927e-04],\n",
      "        [9.9771e-01, 2.2918e-03],\n",
      "        [9.9974e-01, 2.6210e-04],\n",
      "        [9.9959e-01, 4.1470e-04],\n",
      "        [9.9978e-01, 2.1548e-04],\n",
      "        [9.9984e-01, 1.6366e-04],\n",
      "        [9.9966e-01, 3.4452e-04],\n",
      "        [9.9974e-01, 2.5652e-04],\n",
      "        [9.9974e-01, 2.6367e-04],\n",
      "        [9.9975e-01, 2.4498e-04],\n",
      "        [9.9968e-01, 3.2339e-04],\n",
      "        [9.9971e-01, 2.8628e-04],\n",
      "        [9.9979e-01, 2.1070e-04],\n",
      "        [9.9257e-01, 7.4294e-03],\n",
      "        [9.9981e-01, 1.9406e-04],\n",
      "        [9.9981e-01, 1.8616e-04],\n",
      "        [9.9947e-01, 5.3087e-04],\n",
      "        [9.9976e-01, 2.4432e-04],\n",
      "        [9.9977e-01, 2.3011e-04],\n",
      "        [9.9911e-01, 8.8701e-04],\n",
      "        [9.9930e-01, 7.0034e-04],\n",
      "        [9.9983e-01, 1.7162e-04],\n",
      "        [9.9977e-01, 2.2862e-04],\n",
      "        [9.9963e-01, 3.6495e-04],\n",
      "        [9.9984e-01, 1.6381e-04],\n",
      "        [9.9938e-01, 6.1508e-04],\n",
      "        [9.9974e-01, 2.5652e-04],\n",
      "        [9.9979e-01, 2.1033e-04],\n",
      "        [9.9937e-01, 6.3387e-04],\n",
      "        [9.9982e-01, 1.7571e-04],\n",
      "        [9.9973e-01, 2.6959e-04],\n",
      "        [9.9980e-01, 1.9775e-04],\n",
      "        [9.9909e-01, 9.1277e-04],\n",
      "        [9.9912e-01, 8.7591e-04],\n",
      "        [9.9878e-01, 1.2157e-03],\n",
      "        [9.9957e-01, 4.3408e-04],\n",
      "        [9.9984e-01, 1.6243e-04],\n",
      "        [9.9944e-01, 5.6308e-04],\n",
      "        [9.9978e-01, 2.1934e-04],\n",
      "        [9.9279e-01, 7.2120e-03],\n",
      "        [9.9983e-01, 1.7443e-04],\n",
      "        [9.9930e-01, 6.9643e-04],\n",
      "        [9.9967e-01, 3.3018e-04],\n",
      "        [9.9953e-01, 4.6758e-04],\n",
      "        [9.9969e-01, 3.0764e-04],\n",
      "        [9.9966e-01, 3.4046e-04],\n",
      "        [9.9974e-01, 2.6367e-04],\n",
      "        [9.9982e-01, 1.7914e-04],\n",
      "        [9.6953e-01, 3.0465e-02],\n",
      "        [9.9981e-01, 1.9226e-04],\n",
      "        [9.9927e-01, 7.3159e-04],\n",
      "        [9.9983e-01, 1.7208e-04],\n",
      "        [9.9971e-01, 2.8579e-04],\n",
      "        [9.9971e-01, 2.8580e-04],\n",
      "        [9.9141e-01, 8.5863e-03],\n",
      "        [9.9912e-01, 8.7591e-04],\n",
      "        [9.9942e-01, 5.8465e-04],\n",
      "        [9.9971e-01, 2.8755e-04],\n",
      "        [9.9971e-01, 2.8552e-04],\n",
      "        [9.9968e-01, 3.2392e-04],\n",
      "        [9.9941e-01, 5.9205e-04],\n",
      "        [9.9979e-01, 2.1055e-04],\n",
      "        [9.9279e-01, 7.2120e-03],\n",
      "        [9.9963e-01, 3.6498e-04],\n",
      "        [9.9980e-01, 1.9657e-04],\n",
      "        [9.9976e-01, 2.4139e-04],\n",
      "        [9.9972e-01, 2.8247e-04],\n",
      "        [9.9980e-01, 1.9803e-04],\n",
      "        [9.9983e-01, 1.7201e-04],\n",
      "        [9.9981e-01, 1.8883e-04],\n",
      "        [9.9977e-01, 2.2863e-04],\n",
      "        [9.9911e-01, 8.9252e-04],\n",
      "        [9.9983e-01, 1.7183e-04],\n",
      "        [9.9980e-01, 2.0045e-04],\n",
      "        [9.9961e-01, 3.8928e-04],\n",
      "        [9.9858e-01, 1.4168e-03],\n",
      "        [9.9945e-01, 5.5112e-04],\n",
      "        [9.9981e-01, 1.9226e-04],\n",
      "        [9.9955e-01, 4.5339e-04],\n",
      "        [9.9953e-01, 4.7128e-04],\n",
      "        [9.9975e-01, 2.4498e-04],\n",
      "        [9.9971e-01, 2.8755e-04],\n",
      "        [9.9984e-01, 1.6381e-04],\n",
      "        [9.9967e-01, 3.2964e-04],\n",
      "        [9.9968e-01, 3.1726e-04],\n",
      "        [9.9975e-01, 2.4598e-04],\n",
      "        [9.9979e-01, 2.0825e-04],\n",
      "        [9.9930e-01, 6.9643e-04],\n",
      "        [9.9970e-01, 2.9642e-04],\n",
      "        [9.9976e-01, 2.4432e-04],\n",
      "        [9.9981e-01, 1.8648e-04],\n",
      "        [9.9982e-01, 1.7911e-04],\n",
      "        [9.9882e-01, 1.1770e-03],\n",
      "        [9.9980e-01, 1.9594e-04],\n",
      "        [9.9967e-01, 3.3398e-04],\n",
      "        [9.9981e-01, 1.9041e-04],\n",
      "        [9.9980e-01, 1.9657e-04],\n",
      "        [9.9976e-01, 2.3924e-04],\n",
      "        [9.9975e-01, 2.4613e-04],\n",
      "        [9.9981e-01, 1.9155e-04],\n",
      "        [9.9970e-01, 2.9642e-04],\n",
      "        [9.9969e-01, 3.0764e-04],\n",
      "        [9.9976e-01, 2.3924e-04],\n",
      "        [9.9971e-01, 2.8580e-04],\n",
      "        [9.9983e-01, 1.7366e-04],\n",
      "        [9.9976e-01, 2.3899e-04],\n",
      "        [9.9973e-01, 2.6705e-04],\n",
      "        [9.9976e-01, 2.4382e-04],\n",
      "        [9.9978e-01, 2.2105e-04],\n",
      "        [9.9968e-01, 3.2392e-04],\n",
      "        [9.9932e-01, 6.8234e-04],\n",
      "        [9.9877e-01, 1.2303e-03]], grad_fn=<SoftmaxBackward>)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/awl27/python36env/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "softmaxed = torch.nn.functional.softmax(preds['scores'])\n",
    "print(softmaxed)\n",
    "\n",
    "predicted = [t[1].item() for t in softmaxed]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.5775520000000001\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Val acc: 0.502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "print('Test AUROC:', roc_auc_score(y_val, predicted))\n",
    "\n",
    "preds_bin = [1 if i > 0.1 else 0 for i in predicted]\n",
    "print(preds_bin)\n",
    "print('Val acc:', accuracy_score(y_val, preds_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0011254631681367755, 0.00017559983825776726, 0.00022766573238186538, 0.0002962196012958884, 0.00019562843954190612, 0.31385016441345215, 0.0002984744496643543, 0.00028152469894848764, 0.00021215301239863038, 0.00020871087326668203, 0.00016086376854218543, 0.003923343028873205, 0.00025785318575799465, 0.0003990858676843345, 0.0003756936639547348, 0.0004199052054900676, 0.00021891521464567631, 0.0004053784068673849, 0.0004884320078417659, 0.00018312406609766185, 0.00018379895482212305, 0.0016986430855467916, 0.0005126335308887064, 0.0002093981602229178, 0.00023433593742083758, 0.00033807242289185524, 0.0004454247246030718, 0.00020043957920279354, 0.0006219418137334287, 0.013332842849195004, 0.0008582607842981815, 0.00018813024507835507, 0.0002666839864104986, 0.0002897251979447901, 0.0004851285193581134, 0.00029084301786497235, 0.00027539092116057873, 0.0005018523661419749, 0.00023202685406431556, 0.001176820369437337, 0.00030185302603058517, 0.0014672846300527453, 0.00018275078036822379, 0.0002864758134819567, 0.00031247755396179855, 0.0002812046732287854, 0.0003200333740096539, 0.00022634890046902, 0.0001821853220462799, 0.0003338833339512348, 0.00018166251538787037, 0.0010491847060620785, 0.0002283533540321514, 0.0002936333476100117, 0.0005743540823459625, 0.00020663395116571337, 0.0003228757996112108, 0.0002158489660359919, 0.0008051785407587886, 0.00042942474829033017, 0.00018813024507835507, 0.0003886207123287022, 0.00020779510668944567, 0.00035444513196125627, 0.001189010450616479, 0.00036375285708345473, 0.0003368570760358125, 0.00020663395116571337, 0.00020739804313052446, 0.00035021614166907966, 0.0005246628425084054, 0.0005439954111352563, 0.0002785671968013048, 0.0004909337149001658, 0.00029715817072428763, 0.0003363930154591799, 0.0002680099569261074, 0.0003765946312341839, 0.0005474578938446939, 0.00035002941149286926, 0.00021840192493982613, 0.001294137560762465, 0.00028932877467013896, 0.0006498130387626588, 0.00038609272451139987, 0.0007363561890088022, 0.00018813024507835507, 0.00022533575247507542, 0.0012042106827721, 0.00024067776394076645, 0.0003363930154591799, 0.0005426867865025997, 0.003170863026753068, 0.00029637556872330606, 0.0004008074465673417, 0.00045127258636057377, 0.001189010450616479, 0.00020471155585255474, 0.00017753457359503955, 0.0003878545539919287, 0.00017177959671244025, 0.0002321904175914824, 0.00028681906405836344, 0.0002966195170301944, 0.00035002941149286926, 0.0002588268544059247, 0.00045110328937880695, 0.0006498130387626588, 0.0001719884021440521, 0.0005015826318413019, 0.0004909337149001658, 0.00032148740137927234, 0.0012917780550196767, 0.0005233956617303193, 0.00023558193061035126, 0.0004144156409893185, 0.0002522672584746033, 0.0002258612512378022, 0.0005077408859506249, 0.0011137701803818345, 0.00019320913997944444, 0.0005801470251753926, 0.00043266816646791995, 0.00031183057581074536, 0.0002580792352091521, 0.0012237862683832645, 0.00024082556774374098, 0.0003156183229293674, 0.0002404713159194216, 0.0007699790876358747, 0.000681967125274241, 0.0009845292661339045, 0.00036616341094486415, 0.00016008905367925763, 0.00033003033604472876, 0.00020480294188018888, 0.00026145638548769057, 0.0002023124397965148, 0.00019568328571040183, 0.00023356860037893057, 0.0002926590677816421, 0.0003587391984183341, 0.00017663731705397367, 0.00021148614177946, 0.0003508631489239633, 0.00024487939663231373, 0.014468698762357235, 0.000290081457933411, 0.000444465724285692, 0.0003680746885947883, 0.00026451938902027905, 0.000483302166685462, 0.0004444318183232099, 0.00028681906405836344, 0.00019471676205284894, 0.00023141376732382923, 0.0001697956322459504, 0.0002966195170301944, 0.000533873972017318, 0.0006426595500670373, 0.00023003666137810796, 0.0001585078425705433, 0.0008515383815392852, 0.00043266816646791995, 0.0002990399079862982, 0.00034954288275912404, 0.0011404977412894368, 0.00041186538874171674, 0.0003065284981857985, 0.0003496752178762108, 0.00041655063978396356, 0.003070593113079667, 0.000260121189057827, 0.000532572332303971, 0.0004715828108601272, 0.0003949303354602307, 0.00020204274915158749, 0.0005246219225227833, 0.0001978100190171972, 0.001652622246183455, 0.00022007754887454212, 0.0005983772571198642, 0.0008492658962495625, 0.0003592145221773535, 0.00022301710851024836, 0.00019980799697805196, 0.00016380625311285257, 0.000905241584405303, 0.00016008905367925763, 0.0006172736175358295, 0.0007484789821319282, 0.00018808539607562125, 0.0001960655936272815, 0.00026451938902027905, 0.00021371753246057779, 0.00020824495004490018, 0.000285564485238865, 0.0005593912210315466, 0.0005715155857615173, 0.0003090950776822865, 0.0002616611309349537, 0.00022294842347037047, 0.00021433790971059352, 0.0002785671968013048, 0.0008403555257245898, 0.00021164426289033145, 0.0003101601032540202, 0.0006229767459444702, 0.00803202111274004, 0.0033604104537516832, 0.0002911506744567305, 0.0023603420704603195, 0.000616863660980016, 0.00034250563476234674, 0.00080033642007038, 0.0003165150701534003, 0.0005966707831248641, 0.0005327068502083421, 0.00030259121558628976, 0.00028155851759947836, 0.0002436041977489367, 0.00024108127399813384, 0.0005799010396003723, 0.0004083341918885708, 0.00024134850536938757, 0.00028872917755506933, 0.00019038701429963112, 0.00022937719768378884, 0.00021435444068629295, 0.00018306680431123823, 0.0003170831478200853, 0.05981150642037392, 0.00016380625311285257, 0.00022301710851024836, 0.0002588268544059247, 0.0009633004083298147, 0.00080033642007038, 0.0006535819265991449, 0.0003406448813620955, 0.0001779744343366474, 0.00021881838620174676, 0.00027862272690981627, 0.001294137560762465, 0.00016391201643273234, 0.0003728960582520813, 0.0002438219089526683, 0.00031970793497748673, 0.0002953477087430656, 0.00037849516957066953, 0.00020212482195347548, 0.0003824125451501459, 0.0003171917051076889, 0.0003065284981857985, 0.006770030129700899, 0.00015276555495802313, 0.0006877006962895393, 0.0006296682986430824, 0.0003866042825393379, 0.00025013761478476226, 0.00025588442804291844, 0.013828196562826633, 0.00019696535309776664, 0.0006257148925215006, 0.00036614001146517694, 0.0002118439442710951, 0.0003983668575529009, 0.0002876263752114028, 0.000681967125274241, 0.0003133486898150295, 0.000533873972017318, 0.00024346602731384337, 0.0006457747076638043, 0.00021148614177946, 0.00031781464349478483, 0.0004186888982076198, 0.00017663731705397367, 0.000260121189057827, 0.0006064556655474007, 0.00023319898173213005, 0.00020040253002662212, 0.00040487441583536565, 0.0009234788594767451, 0.00025972866569645703, 0.0001980277884285897, 0.0003513933334033936, 0.00019614785560406744, 0.00019083191000390798, 0.00027682509971782565, 0.00023044561385177076, 0.0005817872006446123, 0.00021547965297941118, 0.00028838225989602506, 0.00023898587096482515, 0.0001631230697967112, 0.0007031862623989582, 0.00020450320153031498, 0.00024192771525122225, 0.0004441220371518284, 0.00016990819131024182, 0.00024192771525122225, 0.00028551602736115456, 0.0002065364533336833, 0.00039985316107049584, 0.00019212103507015854, 0.00016449493705295026, 0.0002237337757833302, 0.0002962803700938821, 0.00038927808054722846, 0.00019154597248416394, 0.0066392505541443825, 0.03375096619129181, 0.0002447885926812887, 0.0002500847040209919, 0.0002692208217922598, 0.00020206028420943767, 0.0002601149899419397, 0.0006338701932691038, 0.00019979542412329465, 0.07223381102085114, 0.0001720776199363172, 0.0005138384294696152, 0.0001844981306931004, 0.0002105475141433999, 0.0003224558604415506, 0.0011769564589485526, 0.0003469326184131205, 0.0007031862623989582, 0.0002824743278324604, 0.00022155026090331376, 0.0002301149652339518, 0.0002514546795282513, 0.04228201508522034, 0.00025972866569645703, 0.0002490424958523363, 0.0005685333162546158, 0.0006832782528363168, 0.000551122531760484, 0.0003649478603620082, 0.00030168157536536455, 0.0008925240836106241, 0.00018462096340954304, 0.0001954794570337981, 0.00457480875775218, 0.0002447885926812887, 0.00029006623663008213, 0.0002574776008259505, 0.00021137885050848126, 0.0006792729836888611, 0.00039473664946854115, 0.00028472853591665626, 0.0005846493877470493, 0.00014938972890377045, 0.0007079677889123559, 0.0001977508218260482, 0.00017765176016837358, 0.0003314659697934985, 0.0004340785671956837, 0.00028627869323827326, 0.00039158109575510025, 0.000362062593922019, 0.0005817872006446123, 0.0003032121167052537, 0.0066392505541443825, 0.00039328457205556333, 0.000467582925921306, 0.0027294449973851442, 0.00018293524044565856, 0.00022297883697319776, 0.0001791381073417142, 0.00021724427642766386, 0.00030168157536536455, 0.0005138384294696152, 0.0008870054734870791, 0.07223381102085114, 0.00039158109575510025, 0.00019854280981235206, 0.00019854280981235206, 0.00036031907075084746, 0.013731292448937893, 0.0001704565220279619, 0.0003224558604415506, 0.0002514546795282513, 0.00019979542412329465, 0.00025764357997104526, 0.0004441220371518284, 0.00020916749781463295, 0.0003233923634979874, 0.00028838225989602506, 0.0006792729836888611, 0.0022918153554201126, 0.0002621008607093245, 0.00041470001451671124, 0.00021547965297941118, 0.00016365871124435216, 0.00034451784449629486, 0.0002565214817877859, 0.00026366711244918406, 0.0002449835301376879, 0.0003233923634979874, 0.00028627869323827326, 0.00021069835929665715, 0.007429417222738266, 0.00019406434148550034, 0.0001861584751168266, 0.0005308749387040734, 0.0002443192061036825, 0.0002301149652339518, 0.0008870054734870791, 0.0007003389182500541, 0.0001716158731142059, 0.00022861521574668586, 0.0003649478603620082, 0.00016380625311285257, 0.000615078373812139, 0.0002565214817877859, 0.00021032681979704648, 0.0006338701932691038, 0.00017571057833265513, 0.0002695914590731263, 0.0001977508218260482, 0.0009127715020440519, 0.0008759128395467997, 0.0012156831799075007, 0.0004340785671956837, 0.00016243192658293992, 0.0005630766390822828, 0.00021934081451036036, 0.00721202464774251, 0.00017443104297854006, 0.0006964320782572031, 0.00033017664100043476, 0.000467582925921306, 0.00030763779068365693, 0.0003404635936021805, 0.00026366711244918406, 0.0001791381073417142, 0.030465275049209595, 0.00019226325093768537, 0.0007315907860174775, 0.0001720776199363172, 0.0002857872750610113, 0.00028579929494298995, 0.008586338721215725, 0.0008759128395467997, 0.0005846493877470493, 0.00028754572849720716, 0.00028551602736115456, 0.0003239185025449842, 0.0005920460098423064, 0.0002105475141433999, 0.00721202464774251, 0.0003649772552307695, 0.00019657101074699312, 0.00024138994922395796, 0.0002824743278324604, 0.0001980277884285897, 0.00017200577713083476, 0.00018883041047956795, 0.00022862546029500663, 0.0008925240836106241, 0.00017183332238346338, 0.00020044721895828843, 0.00038927808054722846, 0.0014168324414640665, 0.000551122531760484, 0.00019226325093768537, 0.0004533859610091895, 0.00047127771540544927, 0.0002449835301376879, 0.00028754572849720716, 0.00016380625311285257, 0.00032964322599582374, 0.00031725646113045514, 0.0002459751849528402, 0.00020825011597480625, 0.0006964320782572031, 0.0002964224840980023, 0.0002443192061036825, 0.00018647967954166234, 0.00017910837777890265, 0.0011769564589485526, 0.00019594373588915914, 0.00033398106461390853, 0.00019041169434785843, 0.00019657101074699312, 0.00023923732805997133, 0.0002461314434185624, 0.00019154597248416394, 0.0002964224840980023, 0.00030763779068365693, 0.00023923732805997133, 0.00028579929494298995, 0.00017365554231218994, 0.00023898587096482515, 0.00026704961783252656, 0.00024381911498494446, 0.00022104618255980313, 0.0003239185025449842, 0.0006823393050581217, 0.0012303220573812723]\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualBERT(\n",
       "  (model): VisualBERTForClassification(\n",
       "    (bert): VisualBERTBase(\n",
       "      (embeddings): BertVisioLinguisticEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (token_type_embeddings_visual): Embedding(2, 768)\n",
       "        (position_embeddings_visual): Embedding(512, 768)\n",
       "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Sequential(\n",
       "      (0): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36env",
   "language": "python",
   "name": "python36env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

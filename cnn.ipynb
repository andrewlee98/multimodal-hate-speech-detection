{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import cv2\n",
    "import re\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make image dataloader using flow_from_dataframe\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# load data to extract labels\n",
    "data_dir = 'mmhs150k/'\n",
    "model_dir = 'models/'\n",
    "tweet_dict = json.load(open(data_dir + 'MMHS150K_GT.json', 'r'))\n",
    "\n",
    "# # read split id's and return data generator\n",
    "# def get_data_dict(path):\n",
    "    \n",
    "#     # build dictionary mapping id's to labels\n",
    "#     data = {'id': [], 'label': []}\n",
    "#     for id in open(data_dir + path, 'r').read().splitlines():\n",
    "\n",
    "#         # get majority vote label\n",
    "#         binary_labels = [1 if n > 0 else 0 for n in tweet_dict[id]['labels']]\n",
    "#         label = 1 if sum(binary_labels)/len(tweet_dict[id]['labels']) > 0.5 else 0\n",
    "\n",
    "#         # save to data dict\n",
    "#         data['id'].append(id + '.jpg')\n",
    "#         data['label'].append(str(label))\n",
    "        \n",
    "#     data_df = pd.DataFrame.from_dict(data) # get dataframe to flow from\n",
    "    \n",
    "#     datagen = ImageDataGenerator(rescale=1./255,\n",
    "#                                  samplewise_center=True,\n",
    "#                                  samplewise_std_normalization=True,\n",
    "#                                  width_shift_range=0.3,\n",
    "#                                  height_shift_range=0.3,\n",
    "#                                  shear_range=10,\n",
    "#                                  horizontal_flip=True,\n",
    "#                                  vertical_flip=True)\n",
    "#     datagen = ImageDataGenerator(rescale=1./255)\n",
    "#     generator = datagen.flow_from_dataframe(\n",
    "#         dataframe=data_df,\n",
    "#         directory=data_dir + 'img_resized',\n",
    "#         x_col='id',\n",
    "#         y_col='label',\n",
    "#         target_size=(299, 299),\n",
    "#         batch_size=16,\n",
    "#         class_mode='binary')\n",
    "    \n",
    "#     return generator\n",
    "\n",
    "# train_generator = get_data_dict('splits/train_ids.txt')\n",
    "# val_generator = get_data_dict('splits/val_ids.txt')\n",
    "# test_generator = get_data_dict('splits/test_ids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom data generator to handle cropping\n",
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, splits_path, tweet_dict, batch_size=32, dim=(299, 299), n_channels=3, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # build labels list and id list\n",
    "        self.id_list = open(splits_path, 'r').read().splitlines()\n",
    "        self.labels = dict()\n",
    "        for id in self.id_list:\n",
    "            binary_labels = [1 if n > 0 else 0 for n in tweet_dict[id]['labels']]\n",
    "            label = 1 if sum(binary_labels)/len(tweet_dict[id]['labels']) > 0.5 else 0\n",
    "            self.labels[id] = label\n",
    "            \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.id_list) / self.batch_size)) + 1 # last batch is partial\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:index*self.batch_size + self.batch_size]\n",
    "        \n",
    "        \n",
    "        # Find list of IDs\n",
    "        id_list_temp = [self.id_list[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(id_list_temp)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.id_list))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, id_list_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((len(id_list_temp), *self.dim, self.n_channels))\n",
    "        y = np.empty(len(id_list_temp), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(id_list_temp):\n",
    "            # Store sample\n",
    "            X[i,] = self.process_img(data_dir + 'img_resized/' + ID + '.jpg')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def process_img(self, path): # method for getting image\n",
    "        img = Image.open(path)\n",
    "        img.load()\n",
    "        data = np.asarray(img, dtype='uint8')\n",
    "        im = data[:self.dim[0], :self.dim[1]]\n",
    "        \n",
    "        if im.shape==(self.dim[0], self.dim[1]): im = np.stack((im,)*3, axis=-1) # handle grayscale\n",
    "        \n",
    "        return augment(im)\n",
    "    \n",
    "    def get_labels(self): # get list of labels for calculating AUROC\n",
    "        return [self.labels[ID] for ID in self.id_list]\n",
    "    \n",
    "    def augment(self, im): # random crop and random mirror\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "train_gen = DataGenerator(splits_path=data_dir + 'splits/train_ids.txt',\n",
    "                          tweet_dict=tweet_dict,\n",
    "                          batch_size=32,\n",
    "                          dim=(299, 299),\n",
    "                          n_channels=3,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_gen = DataGenerator(splits_path=data_dir + 'splits/val_ids.txt',\n",
    "                          tweet_dict=tweet_dict,\n",
    "                          batch_size=32,\n",
    "                          dim=(299, 299),\n",
    "                          n_channels=3,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_gen = DataGenerator(splits_path=data_dir + 'splits/test_ids.txt',\n",
    "                          tweet_dict=tweet_dict,\n",
    "                          batch_size=32,\n",
    "                          dim=(299, 299),\n",
    "                          n_channels=3,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 8, 8, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              134218752 \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 156,546,849\n",
      "Trainable params: 134,744,065\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten\n",
    "\n",
    "conv_base = keras.applications.inception_v3.InceptionV3(include_top=False, \n",
    "                                                        weights='imagenet', \n",
    "                                                        input_shape=(299, 299, 3))\n",
    "for layer in conv_base.layers[:-1]: layer.trainable = False # freeze pretrained layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Adam(lr = 0.001)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4214/4214 [==============================] - 2770s 657ms/step - loss: 0.6351 - accuracy: 0.7763 - val_loss: 2.1475 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit_generator(train_gen, \n",
    "                    validation_data=val_gen,\n",
    "                    shuffle=True,\n",
    "                    epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.50019996\n",
      "313/313 [==============================] - 176s 561ms/step\n",
      "Test acc: 0.5001000165939331\n",
      "Test F1: 0.0007995202878273037\n",
      "Test Precision: 0.6666666666666666\n",
      "Test Recall: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "import math\n",
    "\n",
    "y_test = test_gen.get_labels()\n",
    "\n",
    "# get AUROC\n",
    "preds = np.concatenate([model.predict(test_gen.__getitem__(idx)[0]) for idx in range(len(test_gen))])\n",
    "print('Test AUROC:', roc_auc_score(y_test, preds))\n",
    "\n",
    "# get loss and acc\n",
    "print('Test acc:', model.evaluate(test_gen)[1])\n",
    "\n",
    "# get F1\n",
    "preds_bin = np.array(preds)\n",
    "preds_bin[preds>0.5] = 1\n",
    "preds_bin[preds<=0.5] = 0\n",
    "print('Test F1:', f1_score(y_test, preds_bin, zero_division=1))\n",
    "print('Test Precision:', precision_score(y_test, preds_bin, zero_division=1))\n",
    "print('Test Recall:', recall_score(y_test, preds_bin, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir + 'CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see fraction of positive examples\n",
    "print(sum(train_generator.classes)/len(train_generator.classes))\n",
    "print(sum(val_generator.classes)/len(val_generator.classes))\n",
    "print(sum(test_generator.classes)/len(test_generator.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(model_dir + 'CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "print(len(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make image dataloader using flow_from_dataframe\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# load data to extract labels\n",
    "data_dir = 'mmhs150k/'\n",
    "tweet_dict = json.load(open(data_dir + 'MMHS150K_GT.json', 'r'))\n",
    "\n",
    "# read split id's and return data generator\n",
    "def get_data_dict(path):\n",
    "    \n",
    "    # build dictionary mapping id's to labels\n",
    "    data = {'id': [], 'label': []}\n",
    "    for id in open(data_dir + path, 'r').read().splitlines()[:2]: # test for two\n",
    "\n",
    "        # get majority vote label\n",
    "        binary_labels = [1 if n > 0 else 0 for n in tweet_dict[id]['labels']]\n",
    "        label = 1 if sum(binary_labels)/len(tweet_dict[id]['labels']) > 0.5 else 0\n",
    "\n",
    "        # save to data dict\n",
    "        data['id'].append(id + '.jpg')\n",
    "        data['label'].append(\"0\" if not data['label'] else \"1\")\n",
    "        \n",
    "        im = Image.open(data_dir + 'img_resized/' + id + '.jpg')\n",
    "        display(im)\n",
    "        \n",
    "        \n",
    "    data_df = pd.DataFrame.from_dict(data) # get dataframe to flow from\n",
    "    \n",
    "    datagen = ImageDataGenerator()#rescale=1./255)\n",
    "    generator = datagen.flow_from_dataframe(\n",
    "        dataframe=data_df,\n",
    "        directory=data_dir + 'img_resized',\n",
    "        x_col='id',\n",
    "        y_col='label',\n",
    "        target_size=(299, 299),\n",
    "        batch_size=2,\n",
    "        class_mode='binary')\n",
    "    \n",
    "    return generator\n",
    "\n",
    "train_generator = get_data_dict('splits/train_ids.txt')\n",
    "\n",
    "# test image augmentation\n",
    "x, y = train_generator.next()\n",
    "\n",
    "# test image augmentation\n",
    "for img, label in zip(x,y):\n",
    "    im = Image.fromarray(np.uint8(img), 'RGB')\n",
    "    display(im)\n",
    "    im = Image.fromarray(img, 'RGB')\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
